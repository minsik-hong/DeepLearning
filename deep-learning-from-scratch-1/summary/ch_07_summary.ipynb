{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf8009f",
   "metadata": {},
   "source": [
    "# 밑바닥 부터 시작하는 딥러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dd519a",
   "metadata": {},
   "source": [
    "## 목차\n",
    "```\n",
    "7.1 전체 구조 \n",
    "7.2 합성곱 계층 \n",
    "__7.2.1 완전연결 계층의 문제점 \n",
    "__7.2.2 합성곱 연산 \n",
    "__7.2.3 패딩 \n",
    "__7.2.4 스트라이드 \n",
    "__7.2.5 3차원 데이터의 합성곱 연산 \n",
    "__7.2.6 블록으로 생각하기 \n",
    "__7.2.7 배치 처리 \n",
    "7.3 풀링 계층 \n",
    "__7.3.1 풀링 계층의 특징 \n",
    "7.4 합성곱/풀링 계층 구현하기 \n",
    "__7.4.1 4차원 배열 \n",
    "__7.4.2 im2col로 데이터 전개하기 \n",
    "__7.4.3 합성곱 계층 구현하기 \n",
    "__7.4.4 풀링 계층 구현하기 \n",
    "7.5 CNN 구현하기 \n",
    "7.6 CNN 시각화하기 \n",
    "__7.6.1 1번째 층의 가중치 시각화하기 \n",
    "__7.6.2 층 깊이에 따른 추출 정보 변화 \n",
    "7.7 대표적인 CNN \n",
    "__7.7.1 LeNet \n",
    "__7.7.2 AlexNet \n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f6217",
   "metadata": {},
   "source": [
    "# Chapter 7: 합성곱 신경망(CNN) \n",
    "\n",
    "- 합성곱 신경망(Convolutional Neural Network, CNN)\n",
    "    - 이미지 인식, 음성인식 등 다양한 분야에 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25976469",
   "metadata": {},
   "source": [
    "## 7.1 전체 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c68782",
   "metadata": {},
   "source": [
    "- Affine 계층: 완전연결(fully-connected)\n",
    "\n",
    "<img src = \"../deep_learning_images/fig 7-1.png\" width = \"70%\" height = \"70%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01009765",
   "metadata": {},
   "source": [
    "- CNN 구조: 합성곱 계층(Convolutional Layer)과 풀링 계층(pooling layer)이 새로 추가(회색)\n",
    "\n",
    "<img src = \"../deep_learning_images/fig 7-2.png\" width = \"70%\" height = \"70%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4390a01",
   "metadata": {},
   "source": [
    "## 7.2 합성곱 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0413f897",
   "metadata": {},
   "source": [
    "- 패딩(padding)\n",
    "- 스트라이드(stride)\n",
    "- 각 계층 사이에는 3차원 데이터 같이 입체적인 데이터가 흐른다는 점에서 완전연결 신경망과 다르다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1343e9ba",
   "metadata": {},
   "source": [
    "### 7.2.1 완전연결 계층의 문제점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3046060",
   "metadata": {},
   "source": [
    "- '데이터 형상이 무시'된다.\n",
    "    - 3차원 데이터 평탕화 -> 1차원 데이터\n",
    "    - 형상에는 소중한 공간적 정보가 담겨 있다.\n",
    "   \n",
    "- 합성곱 계층은 형상을 유지한다.\n",
    "    - 입출력 데이터 == 특징 맵(feature map)\n",
    "    - 입력 데이터 == 입력 특징 맵(input feature map)\n",
    "    - 출력 데이터 == 출력 특징 맵(output feature map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723a26b4",
   "metadata": {},
   "source": [
    "### 7.2.2 합성곱 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292c158",
   "metadata": {},
   "source": [
    "- 합성곱 계층에서의 **합성곱 연산**. 이미지 처리에서 말하는 **필터 연산**.\n",
    "- 입력과 필터에서 대응하는 원소끼리 곱한 후 그 총합을 구한다(**단일 곱셉-누산, fused multiply-add, FMA**)\n",
    "- 완전 연결 신경망의 가중치 매개변수 -> CNN: 필터의 매개변수 \n",
    "- 완전 연결 신경망의 편향 -> CNN: 편향(1x1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67524a63",
   "metadata": {},
   "source": [
    "<img src = \"../deep_learning_images/fig 7-3.png\" width = \"70%\" height = \"70%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f8896d",
   "metadata": {},
   "source": [
    "<img src = \"../deep_learning_images/fig 7-4.png\" width = \"60%\" height = \"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7afa05",
   "metadata": {},
   "source": [
    "<img src = \"../deep_learning_images/fig 7-5.png\" width = \"60%\" height = \"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759e7126",
   "metadata": {},
   "source": [
    "### 7.2.3 패딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd58164",
   "metadata": {},
   "source": [
    "- 값 채우기 패딩(padding)\n",
    "- 패딩은 주로 출력 크기를 조정할 목적으로 사용한다.(입력 데이터의 공간적 크기를 고정한 채로 다음 계층에 전달 가능)\n",
    "\n",
    "\n",
    "<img src = \"../deep_learning_images/fig 7-6.png\" width = \"60%\" height = \"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cefc99",
   "metadata": {},
   "source": [
    "### 7.2.4 스트라이드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f8f94f",
   "metadata": {},
   "source": [
    "- 필터를 적용하는 위치의 간격: 스트라이드(stride)\n",
    "\n",
    "<img src = \"../deep_learning_images/fig 7-7.png\" width = \"60%\" height = \"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bf53f7",
   "metadata": {},
   "source": [
    "- 입력 크기: (H, W)\n",
    "- 필터 크기: (FH, FW)\n",
    "- 출력 크기: (OH, OW)\n",
    "- 패딩: P\n",
    "- 스트라이드: S\n",
    "- 출력 크기 계산 식\n",
    "\n",
    "<img src = \"../deep_learning_images/e 7.1.png\" width = \"40%\" height = \"40%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db9a95b",
   "metadata": {},
   "source": [
    "### 7.2.5 3차원 데이터의 합성곱 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5951c123",
   "metadata": {},
   "source": [
    "- 채널 까지 고려한 3차원 데이터를 다루는 합성곱 연산\n",
    "- 채널 수와 필터의 채널 수가 같아야 한다. (예: 3개)\n",
    "\n",
    "\n",
    "<img src = \"../deep_learning_images/fig 7-8.png\" width = \"60%\" height = \"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f04c4b5",
   "metadata": {},
   "source": [
    "<img src = \"../deep_learning_images/fig 7-9.png\" width = \"60%\" height = \"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98cecd3",
   "metadata": {},
   "source": [
    "### 7.2.6 블록으로 생각하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd581c2",
   "metadata": {},
   "source": [
    "- (채널, 높이, 너비)\n",
    "- (C, H, W)\n",
    "- (C, FH, FW)\n",
    "\n",
    "<img src = \"../deep_learning_images/fig 7-10.png\" width = \"60%\" height = \"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd16c5bf",
   "metadata": {},
   "source": [
    "- 합성곱 연산의 출력으로 다수의 채널을 보내려면? -> 필터(가중치)를 다수 사용\n",
    "\n",
    "<img src = \"../deep_learning_images/fig 7-11.png\" width = \"60%\" height = \"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a1b314",
   "metadata": {},
   "source": [
    "- 합성곱 연산의 처리 흐름(편향 추가)\n",
    "- (FN, OH, OW) + (FN, 1, 1) -> (FN, OH, OW)\n",
    "- 블록의 대응 채널의 원소 모두에 더해진다 -> 형상이 다른 블록의 덧셈: 넘파이의 브로드캐스트 기능\n",
    "\n",
    "<img src = \"../deep_learning_images/fig 7-12.png\" width = \"60%\" height = \"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ec1842",
   "metadata": {},
   "source": [
    "### 7.2.7 배치 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b4dbd6",
   "metadata": {},
   "source": [
    "- 합성곱 연산의 배치 지원\n",
    "- (데이터 수, 채널 수, 높이, 너비)\n",
    "- (N, C, H, W) -> N회 분의 처리를 한 번에 수행\n",
    "\n",
    "<img src = \"../deep_learning_images/fig 7-13.png\" width = \"60%\" height = \"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93537384",
   "metadata": {},
   "source": [
    "## 7.3 풀링 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b192ebdb",
   "metadata": {},
   "source": [
    "- 풀링: 세로, 가로 방향의 공간을 줄이는 연산\n",
    "- 2x2 최대 풀링(max, pooling)을 스트라이드 2로 처리\n",
    "- 풀링의 윈도우 크기와 스트라이드는 같은 값으로 설정하는 것이 보통\n",
    "\n",
    "<img src = \"../deep_learning_images/fig 7-14.png\" width = \"60%\" height = \"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e6af78",
   "metadata": {},
   "source": [
    "### 7.3.1 풀링 계층의 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e31ea2",
   "metadata": {},
   "source": [
    "1. **학습해야 할 매개 변수가 없다**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eac03f7",
   "metadata": {},
   "source": [
    "2. **채널 수가 변하지 않는다**\n",
    "    - 풀링은 채널 수를 바꾸지 않는다(3 그대로).\n",
    "    \n",
    "    <img src = \"../deep_learning_images/fig 7-15.png\" width = \"60%\" height = \"60%\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833e9008",
   "metadata": {},
   "source": [
    "3. **입력의 변화에 영향을 적게 받는다(강건하다)**\n",
    "    - 입력 데이터가 가로로 1원소만큼 어긋나도 출력은 같다(데이터에 따라서는 다를 수도 있다).\n",
    "\n",
    "    <img src = \"../deep_learning_images/fig 7-16.png\" width = \"60%\" height = \"60%\" >\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece06124",
   "metadata": {},
   "source": [
    "## 7.4 합성곱/풀링 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a0705",
   "metadata": {},
   "source": [
    "### 7.4.1 4차원 배열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2926dae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 높이 28, 너비 28, 채널 1개, 데이터 10개\n",
    "x = np.random.rand(10, 1, 28, 28) # 무작위로 데이터 생성\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbed1df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape # (1, 28, 28), 첫 번째 데이터 접근"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07b084bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1].shape # (1, 28, 28), 두 번째 데이터 접근"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81c14b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.62156868e-01, 6.94914731e-01, 5.53475728e-02, 8.07744132e-01,\n",
       "        7.68996606e-01, 9.41921303e-01, 3.84820109e-01, 8.49753246e-01,\n",
       "        2.20161298e-01, 9.12611838e-01, 8.84206853e-01, 4.03340625e-01,\n",
       "        8.30066065e-01, 7.14502334e-01, 1.25188866e-01, 8.13046533e-01,\n",
       "        8.61105393e-01, 6.72505261e-02, 4.29627470e-01, 4.86924558e-01,\n",
       "        5.46584277e-01, 6.78046715e-01, 5.83393652e-01, 9.46104879e-01,\n",
       "        6.65150319e-01, 1.95775958e-01, 5.95856022e-01, 3.85582896e-01],\n",
       "       [4.71642496e-01, 4.80635723e-01, 7.02108131e-01, 2.54432303e-01,\n",
       "        8.73283766e-01, 7.49078093e-01, 4.82797294e-01, 7.21404496e-01,\n",
       "        4.05468034e-01, 2.29175952e-01, 5.56570044e-01, 6.54385968e-01,\n",
       "        5.93519720e-01, 9.46344896e-02, 3.12118403e-01, 6.06024462e-01,\n",
       "        4.87876666e-01, 2.16238413e-01, 5.86476896e-01, 1.17740492e-01,\n",
       "        2.07315390e-01, 8.49155944e-01, 4.74859287e-02, 4.15016751e-01,\n",
       "        1.75507842e-01, 2.86976188e-01, 2.37663279e-01, 9.89384419e-01],\n",
       "       [5.94500780e-01, 9.61805086e-01, 2.91988837e-01, 4.09877908e-01,\n",
       "        1.80806777e-01, 3.29475744e-01, 2.41331420e-01, 5.27182440e-01,\n",
       "        6.60710269e-01, 4.15363551e-01, 9.60428792e-01, 5.27240461e-01,\n",
       "        3.36368102e-01, 9.09345450e-01, 8.21936298e-01, 3.68940820e-01,\n",
       "        4.89101796e-01, 4.86881331e-02, 3.09646309e-01, 8.04596920e-01,\n",
       "        4.71534071e-01, 2.21288400e-01, 5.56967765e-01, 1.36998026e-02,\n",
       "        4.94691601e-01, 7.92896248e-01, 2.37921208e-01, 3.49933385e-01],\n",
       "       [5.04209475e-01, 5.47525250e-01, 1.67432757e-01, 8.96060740e-01,\n",
       "        7.19948225e-01, 6.44051222e-02, 8.74308527e-01, 2.74028047e-01,\n",
       "        6.32447470e-01, 1.74761997e-01, 3.49822981e-01, 5.77205760e-01,\n",
       "        4.83812970e-01, 8.37793108e-02, 7.68249879e-01, 6.27806835e-01,\n",
       "        9.46757706e-01, 8.40939384e-02, 2.02470756e-01, 8.54626460e-01,\n",
       "        8.39457513e-01, 5.44419589e-01, 7.26334694e-01, 4.27475392e-01,\n",
       "        7.73179122e-01, 6.78673687e-01, 2.94715251e-01, 1.78447655e-01],\n",
       "       [4.02583538e-01, 6.56558670e-01, 8.96378385e-01, 3.02598654e-01,\n",
       "        6.03821566e-01, 9.13616871e-01, 9.70188436e-01, 7.20814699e-01,\n",
       "        8.21940957e-01, 2.42706040e-01, 4.10826733e-01, 5.99655740e-01,\n",
       "        7.28789212e-01, 4.96341412e-01, 5.37983568e-01, 8.50840387e-01,\n",
       "        2.24829058e-01, 9.28841359e-01, 1.16101766e-01, 1.03234167e-01,\n",
       "        4.67124881e-01, 4.14073572e-01, 1.54250038e-01, 5.24339929e-01,\n",
       "        5.41669558e-01, 5.80703723e-01, 7.44426389e-01, 2.33815291e-02],\n",
       "       [4.25729542e-02, 7.39621753e-01, 8.98755523e-01, 9.14861966e-01,\n",
       "        7.01748015e-01, 1.92639731e-02, 5.06785619e-01, 4.88181262e-01,\n",
       "        1.92385999e-01, 2.39351830e-01, 1.38834537e-01, 1.53086078e-01,\n",
       "        2.99138151e-01, 5.88625280e-01, 3.33467879e-01, 7.19965268e-02,\n",
       "        6.65261813e-01, 7.19348723e-01, 7.56659072e-01, 1.67767371e-01,\n",
       "        2.22768812e-02, 8.25034393e-01, 4.22807361e-01, 8.92190136e-01,\n",
       "        7.67929837e-01, 7.46319423e-01, 8.95141577e-01, 3.13781308e-01],\n",
       "       [8.12738561e-01, 2.50084235e-01, 1.56696678e-02, 4.08073085e-01,\n",
       "        4.17121140e-01, 7.86303242e-01, 6.79288877e-01, 1.85084245e-01,\n",
       "        5.57597352e-01, 5.85509416e-01, 7.83835040e-01, 9.39098719e-01,\n",
       "        5.89267976e-01, 4.47769083e-01, 1.44475791e-01, 8.53482110e-01,\n",
       "        5.75395752e-01, 2.81798968e-02, 5.01861011e-01, 7.58218511e-01,\n",
       "        4.32900980e-01, 3.08714049e-01, 8.19157362e-01, 9.69669787e-01,\n",
       "        8.14679163e-01, 5.63189519e-01, 8.46461618e-01, 7.32513697e-01],\n",
       "       [3.23091232e-02, 5.54943587e-01, 2.70685729e-01, 9.44076336e-02,\n",
       "        2.93320689e-01, 4.59173662e-01, 9.37907386e-01, 8.57098509e-01,\n",
       "        9.20075139e-01, 9.50661906e-01, 2.00481258e-01, 4.77473809e-01,\n",
       "        5.66338346e-01, 7.49935308e-02, 9.37262378e-01, 6.77795613e-01,\n",
       "        6.96935900e-01, 3.43692828e-01, 9.65617846e-01, 9.81288904e-01,\n",
       "        9.83859547e-01, 4.94250304e-01, 1.12086184e-02, 2.33821907e-01,\n",
       "        7.71964272e-01, 8.95137908e-01, 4.18479066e-02, 7.59961479e-01],\n",
       "       [4.50608558e-01, 2.63644186e-01, 3.37888204e-01, 9.54354781e-01,\n",
       "        4.77577591e-01, 3.06655213e-01, 1.53082917e-01, 1.80450752e-01,\n",
       "        4.20723677e-01, 6.84316542e-01, 4.33454600e-01, 1.48453961e-01,\n",
       "        8.88311071e-01, 3.53564120e-01, 7.80662544e-01, 9.61037775e-01,\n",
       "        4.98101962e-01, 3.69618563e-01, 1.98542249e-01, 5.99316932e-01,\n",
       "        2.61778393e-01, 8.62518066e-01, 4.92361019e-01, 2.37079520e-01,\n",
       "        5.82977563e-01, 3.99025542e-01, 2.56021055e-02, 2.90723413e-01],\n",
       "       [2.08568603e-01, 6.01270294e-01, 4.97220537e-01, 4.12258708e-01,\n",
       "        8.47610208e-01, 9.86242019e-01, 2.87266126e-01, 7.74453133e-01,\n",
       "        4.24248095e-01, 1.71961082e-01, 5.90318948e-01, 6.24569098e-01,\n",
       "        9.44590507e-01, 1.21871425e-01, 2.75616254e-01, 9.40325865e-01,\n",
       "        4.60238376e-01, 9.37813534e-01, 6.63818505e-01, 2.75303638e-01,\n",
       "        7.37859879e-01, 9.65366066e-01, 1.03885119e-01, 1.18573711e-01,\n",
       "        1.22426277e-01, 5.92943762e-01, 1.65781628e-01, 9.99582071e-02],\n",
       "       [9.69332557e-02, 3.94234815e-01, 7.31339977e-01, 6.25869455e-01,\n",
       "        3.91400371e-01, 6.31551215e-01, 7.87657679e-01, 5.41163439e-01,\n",
       "        8.64737046e-02, 1.13919118e-01, 2.55891073e-01, 1.00352178e-01,\n",
       "        3.65510662e-01, 4.58907884e-01, 3.16182613e-01, 5.73202728e-01,\n",
       "        5.99627549e-01, 7.96399474e-01, 9.81114955e-02, 9.57776141e-01,\n",
       "        4.11524158e-02, 4.08229961e-01, 2.48991604e-01, 8.85407536e-01,\n",
       "        5.77248794e-01, 8.48814825e-01, 4.68490100e-01, 3.56897308e-01],\n",
       "       [5.74224771e-01, 5.92967269e-01, 5.45375084e-01, 6.11067857e-01,\n",
       "        7.89570502e-01, 8.84945288e-01, 6.38389054e-01, 1.69751776e-01,\n",
       "        4.63418693e-01, 4.74059896e-01, 4.08904737e-01, 6.20841547e-01,\n",
       "        1.38098842e-01, 9.04467837e-01, 3.93354332e-01, 3.23011842e-01,\n",
       "        4.88130762e-01, 2.07469984e-01, 8.60084583e-01, 5.96087507e-01,\n",
       "        2.85139211e-01, 1.77856420e-01, 3.45588644e-01, 7.44410080e-01,\n",
       "        3.59405788e-01, 7.65976550e-01, 9.57707365e-02, 1.73807844e-01],\n",
       "       [4.31505884e-01, 7.43350393e-01, 8.08827197e-01, 5.86627252e-01,\n",
       "        8.03067867e-01, 9.23609502e-01, 6.90134841e-01, 8.72637906e-01,\n",
       "        2.82918237e-01, 7.26569716e-01, 9.37082930e-01, 6.26395039e-01,\n",
       "        8.42776133e-01, 2.16419709e-01, 6.77097386e-01, 7.29997798e-01,\n",
       "        4.61197703e-01, 7.26997236e-03, 8.93709841e-01, 5.15246022e-01,\n",
       "        4.80850552e-01, 6.74948209e-01, 9.64612019e-01, 2.45792889e-01,\n",
       "        7.83419112e-01, 9.41084685e-01, 2.36756495e-01, 3.88242032e-01],\n",
       "       [9.61539987e-01, 1.96715082e-01, 8.43181765e-01, 3.80153085e-01,\n",
       "        4.46125576e-01, 2.76336566e-01, 5.18626549e-01, 9.00323340e-01,\n",
       "        2.77203254e-01, 5.53626821e-01, 7.86161883e-01, 2.69397540e-01,\n",
       "        6.11605564e-01, 8.82429031e-01, 8.55741287e-02, 5.13572049e-01,\n",
       "        1.99210682e-01, 8.77807264e-01, 2.42242922e-01, 1.84916996e-01,\n",
       "        3.79717086e-01, 3.35619503e-01, 5.51882047e-01, 6.29107273e-01,\n",
       "        2.64740007e-02, 6.82852241e-01, 6.34531458e-01, 3.27310449e-01],\n",
       "       [1.91644217e-01, 4.73755536e-01, 3.76880737e-01, 8.52436712e-01,\n",
       "        3.02792256e-01, 7.72081011e-01, 4.97692177e-01, 4.83049691e-01,\n",
       "        3.14745387e-02, 2.40313616e-01, 9.29557826e-01, 4.92497191e-01,\n",
       "        8.91998366e-01, 9.02069066e-01, 9.70775823e-01, 1.22175705e-01,\n",
       "        4.57730401e-01, 1.32914470e-02, 7.99706214e-01, 5.30101297e-01,\n",
       "        5.67006702e-01, 4.61417467e-01, 3.44900161e-01, 6.61091856e-01,\n",
       "        6.09451980e-02, 6.53326864e-01, 1.88782350e-01, 5.34522060e-01],\n",
       "       [3.83852874e-01, 6.76397955e-01, 8.31112273e-01, 3.82272771e-01,\n",
       "        2.87732667e-01, 1.06661706e-01, 4.92215854e-01, 9.82014242e-01,\n",
       "        7.60660360e-01, 6.45575385e-01, 4.63488737e-01, 4.55044654e-01,\n",
       "        3.37284007e-01, 6.09569050e-01, 9.69888665e-01, 1.87051924e-01,\n",
       "        3.16974017e-02, 4.56688961e-01, 6.31090746e-02, 7.57984868e-01,\n",
       "        6.45996860e-01, 9.10610237e-01, 2.85269723e-01, 4.85167411e-01,\n",
       "        6.94443086e-01, 9.12046265e-01, 7.82820971e-02, 5.67180515e-01],\n",
       "       [4.29613351e-01, 4.67539845e-01, 4.37763028e-01, 2.07933566e-01,\n",
       "        5.10872049e-01, 8.68210074e-01, 3.68125130e-01, 7.86898262e-01,\n",
       "        7.22688006e-01, 3.15483939e-01, 3.24379667e-01, 2.55234405e-01,\n",
       "        4.04292208e-01, 6.43317401e-01, 7.21886182e-01, 6.71092471e-01,\n",
       "        2.70066668e-01, 6.23263821e-01, 1.30327990e-01, 9.53425723e-01,\n",
       "        9.19763111e-01, 1.92102146e-01, 2.32467726e-01, 7.34695570e-01,\n",
       "        1.14324066e-01, 9.13400572e-01, 8.26460961e-01, 2.55550587e-01],\n",
       "       [2.49208573e-01, 2.40461634e-01, 1.13673337e-01, 3.63488755e-01,\n",
       "        5.85134803e-02, 3.22359921e-02, 2.75178559e-01, 5.99720862e-01,\n",
       "        6.44993744e-01, 9.45386868e-02, 2.67747527e-01, 4.40345801e-01,\n",
       "        5.26257885e-01, 9.30366443e-01, 5.41070423e-01, 3.55189500e-01,\n",
       "        4.05929064e-01, 4.71101580e-01, 5.20031803e-01, 2.78005604e-01,\n",
       "        7.14427553e-01, 7.15030390e-01, 8.97793255e-01, 6.89246967e-01,\n",
       "        8.94470674e-01, 5.69006134e-01, 8.66809783e-01, 9.01393294e-01],\n",
       "       [1.62185091e-01, 6.22972319e-01, 7.58397727e-01, 8.72451041e-01,\n",
       "        1.92880759e-01, 4.53622982e-01, 9.75542591e-01, 4.56268409e-01,\n",
       "        9.35522337e-01, 7.06026704e-01, 9.68555923e-01, 8.09392975e-01,\n",
       "        6.79504281e-01, 7.80213463e-02, 9.19099441e-02, 2.22720056e-01,\n",
       "        9.01996948e-01, 9.68066403e-01, 5.08445033e-01, 1.04805242e-01,\n",
       "        1.65136835e-01, 4.92807113e-01, 6.52303165e-01, 3.27592370e-01,\n",
       "        9.45599263e-01, 6.38763980e-01, 6.24260975e-01, 7.03627948e-01],\n",
       "       [9.16689487e-01, 9.08333612e-01, 9.31380281e-02, 6.22184960e-01,\n",
       "        1.81475290e-01, 1.00746643e-01, 6.37795093e-01, 2.54690710e-01,\n",
       "        8.99143097e-01, 1.34495136e-01, 2.49402067e-01, 4.76029278e-01,\n",
       "        6.51877375e-01, 9.56741058e-01, 4.88477968e-01, 7.30368352e-01,\n",
       "        8.55225409e-01, 8.02350830e-01, 4.28632777e-01, 2.87995803e-01,\n",
       "        8.44650444e-01, 2.64920154e-01, 3.22303229e-01, 2.78149975e-01,\n",
       "        9.95140395e-01, 2.07025956e-01, 4.04222383e-01, 1.07910356e-01],\n",
       "       [2.30827519e-01, 6.13435033e-01, 3.05332262e-01, 8.57547301e-01,\n",
       "        6.27388304e-01, 7.67767010e-01, 5.08627892e-01, 5.34762954e-01,\n",
       "        5.66372391e-01, 9.78634888e-01, 3.78471609e-02, 9.34811998e-01,\n",
       "        3.20577873e-01, 3.12548785e-01, 4.31484848e-01, 4.32621042e-01,\n",
       "        2.35817068e-01, 5.69300699e-01, 2.34645960e-01, 1.47052014e-01,\n",
       "        3.26369213e-01, 5.50714678e-01, 8.04635400e-01, 9.70045785e-01,\n",
       "        9.27941968e-01, 5.16615105e-01, 1.52149070e-01, 4.59698527e-01],\n",
       "       [2.29977177e-02, 2.52797351e-01, 4.37689152e-02, 9.88471689e-01,\n",
       "        8.48371589e-01, 4.08669821e-01, 4.19254708e-01, 4.68789569e-01,\n",
       "        6.56745099e-01, 9.93554005e-01, 4.45138266e-01, 6.25145552e-01,\n",
       "        9.80491640e-01, 3.24391457e-01, 6.00686096e-01, 4.31444880e-02,\n",
       "        6.16178618e-01, 3.82077692e-01, 7.45821039e-01, 4.19446227e-01,\n",
       "        4.31345286e-01, 2.27255275e-01, 3.31195690e-01, 6.45250457e-01,\n",
       "        4.63819401e-01, 3.22524882e-01, 5.09825022e-01, 5.45085338e-01],\n",
       "       [5.76515915e-01, 1.85968304e-01, 8.32030204e-01, 1.97549407e-01,\n",
       "        7.31756458e-01, 5.03879308e-01, 8.21722307e-01, 7.77324057e-01,\n",
       "        7.05767967e-02, 7.33889943e-01, 4.41236952e-01, 3.90016386e-01,\n",
       "        7.54904761e-01, 8.66237045e-01, 3.55415421e-01, 1.11709318e-01,\n",
       "        7.30561476e-01, 6.72657805e-01, 3.72320676e-01, 6.11621783e-01,\n",
       "        8.89553544e-01, 4.12837660e-01, 5.92659637e-01, 5.49225548e-01,\n",
       "        9.04556898e-01, 2.02548990e-02, 3.39182635e-01, 7.87034951e-01],\n",
       "       [9.20107428e-01, 9.94817851e-01, 4.67189342e-01, 1.30286976e-01,\n",
       "        9.61459301e-01, 5.31911253e-02, 4.59088123e-01, 3.98138172e-02,\n",
       "        9.24185971e-01, 2.09657105e-01, 7.05592125e-02, 3.38158128e-02,\n",
       "        6.59745472e-01, 8.88541048e-01, 7.92275826e-01, 6.75635922e-01,\n",
       "        6.92110089e-01, 6.88854717e-01, 7.12587526e-01, 8.21976494e-01,\n",
       "        6.89100101e-01, 8.16122932e-01, 1.39769515e-01, 2.14297405e-01,\n",
       "        8.97443225e-01, 4.51975346e-01, 1.24498481e-01, 9.40242583e-01],\n",
       "       [8.95577203e-01, 8.68982333e-01, 4.65487473e-01, 8.48176856e-01,\n",
       "        8.81279771e-01, 1.45535688e-01, 9.08169055e-01, 2.65867415e-01,\n",
       "        8.33650615e-01, 7.54935682e-01, 8.73590032e-01, 6.11160602e-01,\n",
       "        9.27220356e-01, 2.96409224e-01, 8.67781087e-01, 4.88525139e-03,\n",
       "        9.85697945e-01, 5.44303499e-01, 9.69287691e-01, 9.44414542e-01,\n",
       "        7.79977186e-01, 1.39488991e-02, 3.48634908e-01, 9.07897627e-01,\n",
       "        7.12476404e-01, 2.63544738e-01, 2.42633605e-01, 8.98992877e-01],\n",
       "       [8.43884516e-01, 5.03337771e-01, 4.78869860e-01, 4.39986340e-01,\n",
       "        3.79389370e-01, 9.67391644e-02, 7.50781506e-01, 4.50695671e-01,\n",
       "        4.21868707e-04, 8.02625352e-01, 1.88648084e-01, 6.32037237e-01,\n",
       "        6.82570742e-01, 2.06073450e-01, 7.90551238e-01, 4.22955881e-02,\n",
       "        7.48949384e-01, 2.42780046e-01, 8.89078547e-01, 7.76333926e-01,\n",
       "        9.16954110e-01, 7.04670691e-01, 9.79983248e-01, 8.06218242e-01,\n",
       "        5.74809714e-01, 5.97130923e-01, 7.05338875e-01, 4.92726694e-01],\n",
       "       [9.98781323e-01, 7.09775301e-02, 4.24035202e-01, 2.13727709e-02,\n",
       "        1.97555996e-01, 7.24576226e-01, 9.06240549e-02, 4.48029365e-01,\n",
       "        1.68508956e-01, 7.35080242e-01, 8.11441462e-01, 9.99453236e-01,\n",
       "        5.03125574e-01, 6.67466514e-01, 6.62298213e-01, 2.44628387e-01,\n",
       "        3.26298639e-01, 8.14860836e-01, 8.41787221e-01, 5.96195188e-01,\n",
       "        9.86769538e-01, 7.69721047e-01, 4.09264339e-02, 5.76871055e-01,\n",
       "        9.00567900e-01, 6.76871214e-01, 3.49746146e-01, 7.04265655e-01],\n",
       "       [3.89427469e-01, 3.11386798e-01, 9.54619408e-01, 2.06313653e-01,\n",
       "        2.51408308e-01, 5.27548292e-01, 6.52623076e-01, 5.18754605e-01,\n",
       "        1.24446604e-01, 9.13707308e-01, 8.48512271e-01, 1.34723089e-01,\n",
       "        3.38005381e-01, 5.19632051e-01, 6.56854641e-01, 9.52120739e-01,\n",
       "        8.27474759e-01, 7.62102512e-02, 1.51301026e-01, 1.91024495e-02,\n",
       "        5.31822258e-01, 6.91152013e-01, 2.79504256e-01, 1.60366361e-01,\n",
       "        3.75537984e-01, 8.60191492e-01, 4.84133302e-01, 9.66292855e-01]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0] # 첫 번째 데이터의 첫 채널 공간 접근"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cd041a",
   "metadata": {},
   "source": [
    "### 7.4.2 im2col로 데이터 전개하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d65f42",
   "metadata": {},
   "source": [
    "- for 문 대신 **im2col**이라는 편의 함수 사용\n",
    "- im2col: 입력 데이터를 필터링(가중치 계산)하기 좋게 전개하는(펼치는) 함수\n",
    "    - 3차원 입력 데이터에 im2col을 적용 -> 2차원 행렬(정확히는 배치 안의 데이터 수까지 포함 4 -> 2)\n",
    "    \n",
    "    <img src = \"../deep_learning_images/fig 7-17.png\" width = \"60%\" height = \"60%\" >\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a61fcf",
   "metadata": {},
   "source": [
    "- 전개를 필터를 적용하는 모든 영역에 수행하는 게 im2col\n",
    "    - 필터 적용 영역을 앞에서부터 순서대로 1출로 펼친다.\n",
    "    \n",
    "    <img src = \"../deep_learning_images/fig 7-18.png\" width = \"60%\" height = \"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861fa844",
   "metadata": {},
   "source": [
    "- 실제 상황에서는 영역이 겹치는 경우가 대부분(스트라이드에 따라) -> 메모리를 더 많이 소비하는 단점 존재\n",
    "- 컴퓨터는 큰 행렬을 묶어서 계산하는 데 탁월."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e624218",
   "metadata": {},
   "source": [
    "- 합성곱 연산의 필터 처리 상세 과정: 필터를 세로로 1열로 전개하고, im2col이 전개한 데이터와 행렬 곱을 계산한다. 마지막으로 출력 데이터를 변형(reshape)한다(2차원->4차원).\n",
    "\n",
    "\n",
    "<img src = \"../deep_learning_images/fig 7-19.png\" width = \"60%\" height = \"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da21007",
   "metadata": {},
   "source": [
    "### 7.4.3 합성곱 계층 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c942fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "im2col(input_data, filter_h, filter_w, stride=1, pad=0)\n",
    "# input_data - (데이터 수, 채널 수, 높이, 너비)의 4차원 배열로 이뤄진 입력 데이터\n",
    "# filter_h - 필터의 높이\n",
    "# filter_w - 필터의 너비\n",
    "# stride - 스트라이드\n",
    "# pad - 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc3f9588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.util import im2col\n",
    "\n",
    "x1 = np.random.rand(1, 3, 7, 7) # 데이터 수, 채널 수, 높이, 너비\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape) # (9, 75)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7) # 데이터 10개\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape) # (90, 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b61e1",
   "metadata": {},
   "source": [
    "- 2번째 차원의 원소 수 75 동일: 채널 3개, 5x5 데이터\n",
    "- 배치 크기 1: 9 -> 10배: 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e877aff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad) # 입력 데이터를 im2col로 전개\n",
    "        col_W = self.W.reshape(FN, -1).T # 필터 reshape을 사용해 2차원 전개(-1 -> 다차원 배열의 원소 수가 변환 후에도 똑같이 유지되도록 묶어줌)\n",
    "        out = np.dot(col, col_W) + self.b # 전개한 두 행렬의 곱\n",
    "\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2) # 축 순서 변경\n",
    "        \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad) # im2col 의 역 col2im\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b43068",
   "metadata": {},
   "source": [
    "- 필터(가중치), 편향, 스트라이드, 패딩을 인수로 받아 초기화\n",
    "- 필터는 -> (FN, C, FH, FW)의 4차원 형상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d83dc4",
   "metadata": {},
   "source": [
    "- 넘파이의 transpose 함수로 축 순서 변경하기: 인덱스(번호)로 축의 순서를 변경한다.\n",
    "    \n",
    "    <img src = \"../deep_learning_images/fig 7-20.png\" width = \"60%\" height = \"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe2ae06",
   "metadata": {},
   "source": [
    "### 7.4.4 풀링 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6422443d",
   "metadata": {},
   "source": [
    "- im2col을 사용해 입력 데이터 전개. 채널 쪽이 독립적\n",
    "\n",
    "- 폴링 계층 구현 단계\n",
    "    1. 입력 데이터를 전개한다.\n",
    "    2. 행별 최댓값을 구한다.\n",
    "    3. 적절한 모양으로 성형한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c5c79c",
   "metadata": {},
   "source": [
    "- 입력 데이터에 풀링 적용 영역을 전개 (2x2 풀리의 예)\n",
    "\n",
    "<img src = \"../deep_learning_images/fig 7-21.png\" width = \"50%\" height = \"50%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab02cdc",
   "metadata": {},
   "source": [
    "- 풀링 계층 구현의 흐름: 풀링 적용 영역에서 가장 큰 원소는 회색으로 표시\n",
    "\n",
    "<img src = \"../deep_learning_images/fig 7-22.png\" width = \"60%\" height = \"60%\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77945f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        # 전개 (1)\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "        \n",
    "        # 최댓값 (2)\n",
    "        out = np.max(col, axis = 1)\n",
    "        \n",
    "        # 성형 (3)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad) # im2col 의 역 col2im\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a7b31c",
   "metadata": {},
   "source": [
    "### 7.5 CNN 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13f4a2c",
   "metadata": {},
   "source": [
    "- 단순한 CNN의 네트워크 구성\n",
    "\n",
    "\n",
    "<img src = \"../deep_learning_images/fig 7-23.png\" width = \"60%\" height = \"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d7a81f",
   "metadata": {},
   "source": [
    "- 초기화(__init__)때 받는 인수\n",
    "    - input_dim - 입력 데이터(채널 수, 높이, 너비)의 차원\n",
    "    - conv_param - 합성곱 계층의 파라미터(딕셔너리), 딕셔너리의 키는 다음과 같다.\n",
    "        - filter_num - 필터 수\n",
    "        - filter_size - 필터 크기\n",
    "        - stride - 스트라이드\n",
    "        - pad - 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44144226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"단순한 합성곱 신경망\n",
    "    \n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        # 합성곱 계층의 출력 크기 계산\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        # 첫 번째 층의 합성 곱 계층\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        # 두 번째 층의 완전연결 계층\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        # 세 번째 층의 완전연결 계층\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss() # softmaxwithloss 계층 별도 변수에 저장\n",
    "\n",
    "    # 추론 수행\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다（수치미분）.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(오차역전파법).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0dda23e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3004407903319524\n",
      "=== epoch:1, train acc:0.241, test acc:0.195 ===\n",
      "train loss:2.298235438891769\n",
      "train loss:2.2934689112643167\n",
      "train loss:2.2889050668584736\n",
      "train loss:2.2824765213161173\n",
      "train loss:2.2643404122448834\n",
      "train loss:2.247801349536532\n",
      "train loss:2.225060360858545\n",
      "train loss:2.223208194425145\n",
      "train loss:2.1819582992265625\n",
      "train loss:2.149831563003987\n",
      "train loss:2.113925539148518\n",
      "train loss:2.062123086959231\n",
      "train loss:2.030245698177762\n",
      "train loss:1.987115181228056\n",
      "train loss:1.98583438797304\n",
      "train loss:1.836000465247398\n",
      "train loss:1.7690233243870772\n",
      "train loss:1.6392042426331526\n",
      "train loss:1.497249771216472\n",
      "train loss:1.5383125296731506\n",
      "train loss:1.4462881856331087\n",
      "train loss:1.409678480400482\n",
      "train loss:1.1685443263190485\n",
      "train loss:1.2458497749331132\n",
      "train loss:1.1213587337675968\n",
      "train loss:1.0083210636067719\n",
      "train loss:0.951351243251174\n",
      "train loss:0.9313271149655009\n",
      "train loss:0.8958303133171824\n",
      "train loss:0.7919810109129729\n",
      "train loss:0.7746049480360163\n",
      "train loss:0.8575235650362543\n",
      "train loss:0.7504138423764468\n",
      "train loss:0.8786744479090791\n",
      "train loss:0.7495123684865854\n",
      "train loss:0.6501067991229867\n",
      "train loss:0.5847061776518513\n",
      "train loss:0.67706137761422\n",
      "train loss:0.4962929563182773\n",
      "train loss:0.705534313807297\n",
      "train loss:0.5414970028390649\n",
      "train loss:0.6281865399994639\n",
      "train loss:0.518346076268447\n",
      "train loss:0.5719344115518681\n",
      "train loss:0.47066186128642884\n",
      "train loss:0.5803499677046646\n",
      "train loss:0.4513778000158236\n",
      "train loss:0.5580180289488296\n",
      "train loss:0.5377143799640656\n",
      "train loss:0.61115573462794\n",
      "=== epoch:2, train acc:0.819, test acc:0.817 ===\n",
      "train loss:0.4008076530068436\n",
      "train loss:0.5049789284357796\n",
      "train loss:0.7606330433952947\n",
      "train loss:0.543190072427195\n",
      "train loss:0.4039796982211276\n",
      "train loss:0.4781281298273439\n",
      "train loss:0.5173327031166802\n",
      "train loss:0.526819280524225\n",
      "train loss:0.4555794392271881\n",
      "train loss:0.41604727738112246\n",
      "train loss:0.5510171506403952\n",
      "train loss:0.5581502804545027\n",
      "train loss:0.3489259770457895\n",
      "train loss:0.4067317959034506\n",
      "train loss:0.39440956893102586\n",
      "train loss:0.39153391282429695\n",
      "train loss:0.3990077548218433\n",
      "train loss:0.3284789968662702\n",
      "train loss:0.46661590768663946\n",
      "train loss:0.4142234891389148\n",
      "train loss:0.3073142178411227\n",
      "train loss:0.4799073209178994\n",
      "train loss:0.4329812720967336\n",
      "train loss:0.4637846833863233\n",
      "train loss:0.293313844553946\n",
      "train loss:0.3991129445297755\n",
      "train loss:0.6152230154200575\n",
      "train loss:0.32924792232264716\n",
      "train loss:0.40799049451520164\n",
      "train loss:0.360277156164454\n",
      "train loss:0.5700519875328437\n",
      "train loss:0.5422988604983006\n",
      "train loss:0.5528225507271392\n",
      "train loss:0.3978483051007716\n",
      "train loss:0.32350978401616737\n",
      "train loss:0.30325046303028447\n",
      "train loss:0.44265992396813947\n",
      "train loss:0.47287858552976125\n",
      "train loss:0.340773241285547\n",
      "train loss:0.2540244644876465\n",
      "train loss:0.41477147962084054\n",
      "train loss:0.31719848410246326\n",
      "train loss:0.624292936225692\n",
      "train loss:0.30917891013206084\n",
      "train loss:0.2905567679243167\n",
      "train loss:0.4274497392563952\n",
      "train loss:0.31518520499680724\n",
      "train loss:0.288999907224324\n",
      "train loss:0.3080668774010527\n",
      "train loss:0.25434009049050055\n",
      "=== epoch:3, train acc:0.869, test acc:0.839 ===\n",
      "train loss:0.3342366599423921\n",
      "train loss:0.41283473869899295\n",
      "train loss:0.32847887532145215\n",
      "train loss:0.3107101308233854\n",
      "train loss:0.2515977307919878\n",
      "train loss:0.33800596451607084\n",
      "train loss:0.36948100853939175\n",
      "train loss:0.3229347585315308\n",
      "train loss:0.34811681594912963\n",
      "train loss:0.2959142771083955\n",
      "train loss:0.24925173714057997\n",
      "train loss:0.30603967586857717\n",
      "train loss:0.20980443754275624\n",
      "train loss:0.4762137155920351\n",
      "train loss:0.2277973073093734\n",
      "train loss:0.3881884692052251\n",
      "train loss:0.2111401314626927\n",
      "train loss:0.2605040015229425\n",
      "train loss:0.363787638636201\n",
      "train loss:0.39610552723185743\n",
      "train loss:0.2346402434927433\n",
      "train loss:0.25357328954484004\n",
      "train loss:0.29935703636116245\n",
      "train loss:0.30605093396587074\n",
      "train loss:0.3326081244604138\n",
      "train loss:0.3563049955252028\n",
      "train loss:0.22337425667198332\n",
      "train loss:0.32420031777307057\n",
      "train loss:0.2981571695657343\n",
      "train loss:0.34494125354258204\n",
      "train loss:0.32297011828508687\n",
      "train loss:0.3086255817482565\n",
      "train loss:0.42624929763145863\n",
      "train loss:0.29845693047691113\n",
      "train loss:0.4521378113656363\n",
      "train loss:0.21356788267383872\n",
      "train loss:0.29816302218934543\n",
      "train loss:0.20779747301751608\n",
      "train loss:0.38661151288041995\n",
      "train loss:0.3732908392021731\n",
      "train loss:0.29191749365592046\n",
      "train loss:0.2098341283461385\n",
      "train loss:0.15769033185346573\n",
      "train loss:0.37278507076225686\n",
      "train loss:0.3879909265683925\n",
      "train loss:0.34056844315608226\n",
      "train loss:0.4430624004480685\n",
      "train loss:0.30291501440482377\n",
      "train loss:0.370496604554027\n",
      "train loss:0.330278838126457\n",
      "=== epoch:4, train acc:0.892, test acc:0.885 ===\n",
      "train loss:0.45630626264410057\n",
      "train loss:0.3895866059287463\n",
      "train loss:0.2812570595503534\n",
      "train loss:0.2677106173002563\n",
      "train loss:0.3362075579539717\n",
      "train loss:0.24900461294611703\n",
      "train loss:0.28478054106216744\n",
      "train loss:0.2199124390972443\n",
      "train loss:0.34237965342188553\n",
      "train loss:0.273929322950539\n",
      "train loss:0.28238766972259827\n",
      "train loss:0.4452209909688618\n",
      "train loss:0.3928833756029633\n",
      "train loss:0.4539504826096081\n",
      "train loss:0.3335138173861879\n",
      "train loss:0.2522533742891771\n",
      "train loss:0.29295038994419986\n",
      "train loss:0.29134303969396547\n",
      "train loss:0.23316460179617837\n",
      "train loss:0.3741666080986176\n",
      "train loss:0.4229242285315455\n",
      "train loss:0.2038590535176777\n",
      "train loss:0.26191686298177613\n",
      "train loss:0.2896481669592596\n",
      "train loss:0.24703484267845444\n",
      "train loss:0.25570115441031777\n",
      "train loss:0.28165296973484305\n",
      "train loss:0.23630438482323524\n",
      "train loss:0.25497788313249264\n",
      "train loss:0.14696871542185738\n",
      "train loss:0.2221863997030188\n",
      "train loss:0.32785090339638373\n",
      "train loss:0.2020127279192483\n",
      "train loss:0.3410676687091877\n",
      "train loss:0.3498988843864709\n",
      "train loss:0.18159673306871288\n",
      "train loss:0.22767241356667067\n",
      "train loss:0.30004132547633466\n",
      "train loss:0.24040300540903334\n",
      "train loss:0.23239222411514998\n",
      "train loss:0.2194065314981142\n",
      "train loss:0.3989586418653594\n",
      "train loss:0.1995980730019906\n",
      "train loss:0.2719702827977851\n",
      "train loss:0.30092666443303207\n",
      "train loss:0.24004777052499426\n",
      "train loss:0.2853588071488436\n",
      "train loss:0.12350419284488788\n",
      "train loss:0.22801078131130711\n",
      "train loss:0.24332198545932043\n",
      "=== epoch:5, train acc:0.906, test acc:0.895 ===\n",
      "train loss:0.18161467075529594\n",
      "train loss:0.2650949558008227\n",
      "train loss:0.3201681403022622\n",
      "train loss:0.26575588567676917\n",
      "train loss:0.23042820466783212\n",
      "train loss:0.24264678109227625\n",
      "train loss:0.30335714683988835\n",
      "train loss:0.2629026303868308\n",
      "train loss:0.22414415604478063\n",
      "train loss:0.2883778370846985\n",
      "train loss:0.2090800596687911\n",
      "train loss:0.32072896541926044\n",
      "train loss:0.11568488421973017\n",
      "train loss:0.21446521069575192\n",
      "train loss:0.3764152673752609\n",
      "train loss:0.27790497063717046\n",
      "train loss:0.26204806097579725\n",
      "train loss:0.28328910688903663\n",
      "train loss:0.23248446776744905\n",
      "train loss:0.18127200196330892\n",
      "train loss:0.19335982044745262\n",
      "train loss:0.21615028068335695\n",
      "train loss:0.29613486465182715\n",
      "train loss:0.22462284643197392\n",
      "train loss:0.23113166149823822\n",
      "train loss:0.3822368212479113\n",
      "train loss:0.1525604439380453\n",
      "train loss:0.1288928238641778\n",
      "train loss:0.20383569944591415\n",
      "train loss:0.24496859863889056\n",
      "train loss:0.28561266249600964\n",
      "train loss:0.17150214726867655\n",
      "train loss:0.15476538374360993\n",
      "train loss:0.24579131192587847\n",
      "train loss:0.20569654381861058\n",
      "train loss:0.15160022650667382\n",
      "train loss:0.18831884718746805\n",
      "train loss:0.2152070588210425\n",
      "train loss:0.15718467263171174\n",
      "train loss:0.1936515569188232\n",
      "train loss:0.30166754004318735\n",
      "train loss:0.18917812163931783\n",
      "train loss:0.24167457019625502\n",
      "train loss:0.22017875916724164\n",
      "train loss:0.1951264317985192\n",
      "train loss:0.18498411452652333\n",
      "train loss:0.1205827669131861\n",
      "train loss:0.14433910814451248\n",
      "train loss:0.13772596793589476\n",
      "train loss:0.23198471755839403\n",
      "=== epoch:6, train acc:0.919, test acc:0.906 ===\n",
      "train loss:0.18951125396427035\n",
      "train loss:0.22335741336237672\n",
      "train loss:0.14410250314930714\n",
      "train loss:0.05167609430671005\n",
      "train loss:0.22930312993879787\n",
      "train loss:0.1954115728469643\n",
      "train loss:0.1278592357394782\n",
      "train loss:0.3050509249220081\n",
      "train loss:0.21075729748362718\n",
      "train loss:0.15251330431679835\n",
      "train loss:0.22633280600702194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.1117532398809369\n",
      "train loss:0.1830352427941626\n",
      "train loss:0.17976404447538155\n",
      "train loss:0.10560914643852008\n",
      "train loss:0.1736240321643905\n",
      "train loss:0.18580088400329228\n",
      "train loss:0.3114896649043993\n",
      "train loss:0.267242240041093\n",
      "train loss:0.20226875298770142\n",
      "train loss:0.24867321958583843\n",
      "train loss:0.1818294924160532\n",
      "train loss:0.20211600206941405\n",
      "train loss:0.19537444649602825\n",
      "train loss:0.13843621249544555\n",
      "train loss:0.24156649415342307\n",
      "train loss:0.21385984473415345\n",
      "train loss:0.15849650385541825\n",
      "train loss:0.22050243194592592\n",
      "train loss:0.2206939820222548\n",
      "train loss:0.17316298762797622\n",
      "train loss:0.18399222681113747\n",
      "train loss:0.2563935374190395\n",
      "train loss:0.1401488974041205\n",
      "train loss:0.17187273768887593\n",
      "train loss:0.19945664335527793\n",
      "train loss:0.15908902443002912\n",
      "train loss:0.24800306881708964\n",
      "train loss:0.1785873431718688\n",
      "train loss:0.27648734106707484\n",
      "train loss:0.09223572159640099\n",
      "train loss:0.12525541781886612\n",
      "train loss:0.10369709574258616\n",
      "train loss:0.3069772957464841\n",
      "train loss:0.1513653589527012\n",
      "train loss:0.18425483114843608\n",
      "train loss:0.2008345320047995\n",
      "train loss:0.10026265612320064\n",
      "train loss:0.12431200215731746\n",
      "train loss:0.2381643242131034\n",
      "=== epoch:7, train acc:0.932, test acc:0.901 ===\n",
      "train loss:0.15745736272987235\n",
      "train loss:0.19213727604842457\n",
      "train loss:0.13672977975222333\n",
      "train loss:0.1623136617647765\n",
      "train loss:0.10853071226616649\n",
      "train loss:0.11674209246340674\n",
      "train loss:0.11062931039766614\n",
      "train loss:0.1472322257009759\n",
      "train loss:0.1401256228492356\n",
      "train loss:0.21846166788313642\n",
      "train loss:0.10344422781328447\n",
      "train loss:0.17666470317725355\n",
      "train loss:0.24175570901554608\n",
      "train loss:0.07422916577350056\n",
      "train loss:0.12234486746002043\n",
      "train loss:0.1193847782704256\n",
      "train loss:0.20014154944380116\n",
      "train loss:0.3079310658098982\n",
      "train loss:0.08952615137283787\n",
      "train loss:0.13255722419750302\n",
      "train loss:0.15635761885156677\n",
      "train loss:0.11868068418448566\n",
      "train loss:0.21308678331657233\n",
      "train loss:0.18861270305476277\n",
      "train loss:0.1675329009971859\n",
      "train loss:0.09274551860910284\n",
      "train loss:0.1503047146056514\n",
      "train loss:0.27232544716139573\n",
      "train loss:0.12096926435637996\n",
      "train loss:0.0681941231583184\n",
      "train loss:0.17248426919200835\n",
      "train loss:0.23382461809932104\n",
      "train loss:0.1918789180647412\n",
      "train loss:0.07651500499600032\n",
      "train loss:0.15593339855162877\n",
      "train loss:0.18626153952277635\n",
      "train loss:0.10669450408057297\n",
      "train loss:0.19843780037915157\n",
      "train loss:0.21042884679609763\n",
      "train loss:0.11925212286940276\n",
      "train loss:0.15252727375512534\n",
      "train loss:0.1666998354988377\n",
      "train loss:0.20072050375368408\n",
      "train loss:0.15026002436744385\n",
      "train loss:0.09809956389121263\n",
      "train loss:0.09324785275529786\n",
      "train loss:0.2613303457723336\n",
      "train loss:0.14680857040783432\n",
      "train loss:0.12694131227737432\n",
      "train loss:0.1019398747688723\n",
      "=== epoch:8, train acc:0.944, test acc:0.925 ===\n",
      "train loss:0.16380567156634376\n",
      "train loss:0.2893343574033517\n",
      "train loss:0.10806295873856288\n",
      "train loss:0.19426281034418832\n",
      "train loss:0.15420640856483595\n",
      "train loss:0.17460541623506512\n",
      "train loss:0.11407212357300173\n",
      "train loss:0.13533069828389346\n",
      "train loss:0.11870122134022827\n",
      "train loss:0.13467334074014184\n",
      "train loss:0.09797838180670924\n",
      "train loss:0.15932750551670047\n",
      "train loss:0.13435310353167965\n",
      "train loss:0.12203352643326694\n",
      "train loss:0.17867471296672352\n",
      "train loss:0.11786067144840073\n",
      "train loss:0.1025842912979038\n",
      "train loss:0.13642420000643243\n",
      "train loss:0.12931597927463362\n",
      "train loss:0.19436047076790014\n",
      "train loss:0.23074261295649595\n",
      "train loss:0.15629334456269353\n",
      "train loss:0.16718183233186842\n",
      "train loss:0.1253527003586873\n",
      "train loss:0.18735442063626653\n",
      "train loss:0.13028622696136755\n",
      "train loss:0.07719323066744878\n",
      "train loss:0.15208448092297097\n",
      "train loss:0.21996771198957074\n",
      "train loss:0.055096237771906226\n",
      "train loss:0.15728301129304062\n",
      "train loss:0.08514750772234768\n",
      "train loss:0.16062032248230682\n",
      "train loss:0.09526304396198652\n",
      "train loss:0.13496662208502141\n",
      "train loss:0.14578098150259663\n",
      "train loss:0.1691529041549154\n",
      "train loss:0.2509649784583891\n",
      "train loss:0.06260670151121961\n",
      "train loss:0.05615253470909662\n",
      "train loss:0.15039758228637817\n",
      "train loss:0.0752653924092229\n",
      "train loss:0.13484369058528822\n",
      "train loss:0.13639464463726417\n",
      "train loss:0.10888314002202107\n",
      "train loss:0.2930906477363723\n",
      "train loss:0.1805458953630392\n",
      "train loss:0.11490146669174818\n",
      "train loss:0.19253207363147304\n",
      "train loss:0.17392694529009795\n",
      "=== epoch:9, train acc:0.954, test acc:0.937 ===\n",
      "train loss:0.083076249297969\n",
      "train loss:0.25678608954135357\n",
      "train loss:0.08913359023367598\n",
      "train loss:0.18475514305061796\n",
      "train loss:0.08468975271922291\n",
      "train loss:0.12465540064153334\n",
      "train loss:0.09179973973021277\n",
      "train loss:0.19106684009041291\n",
      "train loss:0.14478816744217496\n",
      "train loss:0.11349105885726196\n",
      "train loss:0.07954261852629488\n",
      "train loss:0.16941357785941236\n",
      "train loss:0.10055447394311132\n",
      "train loss:0.10356555622037882\n",
      "train loss:0.0984457074623741\n",
      "train loss:0.0679376512390884\n",
      "train loss:0.1557406060870598\n",
      "train loss:0.06026429883788749\n",
      "train loss:0.08619840647010425\n",
      "train loss:0.08846337214860048\n",
      "train loss:0.14417422189154025\n",
      "train loss:0.11076599593652543\n",
      "train loss:0.08354552976490572\n",
      "train loss:0.07644694311298075\n",
      "train loss:0.15903252595727155\n",
      "train loss:0.15307449348839094\n",
      "train loss:0.18933060033117305\n",
      "train loss:0.11596275689090457\n",
      "train loss:0.11004456478526402\n",
      "train loss:0.07777149809162952\n",
      "train loss:0.07728931055374351\n",
      "train loss:0.10292806782383618\n",
      "train loss:0.05988441094016613\n",
      "train loss:0.13318689762729868\n",
      "train loss:0.08739423085898933\n",
      "train loss:0.06191034565518822\n",
      "train loss:0.1774479163791297\n",
      "train loss:0.09264099988529756\n",
      "train loss:0.07251655120773938\n",
      "train loss:0.15376290493957542\n",
      "train loss:0.04884872065883348\n",
      "train loss:0.05729871194332353\n",
      "train loss:0.05328850206170894\n",
      "train loss:0.09158228838461925\n",
      "train loss:0.12812014840910238\n",
      "train loss:0.0804501320619989\n",
      "train loss:0.07764039817560985\n",
      "train loss:0.14353881462767887\n",
      "train loss:0.08019563272045253\n",
      "train loss:0.05393509777761587\n",
      "=== epoch:10, train acc:0.95, test acc:0.946 ===\n",
      "train loss:0.043406556563519286\n",
      "train loss:0.0894263325498859\n",
      "train loss:0.08074073766863749\n",
      "train loss:0.12239851618185345\n",
      "train loss:0.07228729258474027\n",
      "train loss:0.06109458276713885\n",
      "train loss:0.12365522890532107\n",
      "train loss:0.05995801285649101\n",
      "train loss:0.06400178932761187\n",
      "train loss:0.11005319945247774\n",
      "train loss:0.0974135269245911\n",
      "train loss:0.06809615821187308\n",
      "train loss:0.09505672280917807\n",
      "train loss:0.2685505901491083\n",
      "train loss:0.06573062885579818\n",
      "train loss:0.057977903469410806\n",
      "train loss:0.0563743970435178\n",
      "train loss:0.15352101054918676\n",
      "train loss:0.030552040964784317\n",
      "train loss:0.098887836496192\n",
      "train loss:0.1908802167029114\n",
      "train loss:0.07031604646854314\n",
      "train loss:0.039506556913995364\n",
      "train loss:0.12011352501775363\n",
      "train loss:0.05529672938770438\n",
      "train loss:0.058093057887707966\n",
      "train loss:0.08166975886788565\n",
      "train loss:0.10561671788252251\n",
      "train loss:0.06837623013135466\n",
      "train loss:0.17285294692116035\n",
      "train loss:0.09058545286822048\n",
      "train loss:0.07629107332044606\n",
      "train loss:0.12084244534105158\n",
      "train loss:0.07162901432572807\n",
      "train loss:0.16272482379908748\n",
      "train loss:0.04104380061394061\n",
      "train loss:0.19304836001354095\n",
      "train loss:0.09371425277099028\n",
      "train loss:0.0918018098875945\n",
      "train loss:0.07220168156387656\n",
      "train loss:0.2568096802068205\n",
      "train loss:0.04557020625582033\n",
      "train loss:0.06110545476594435\n",
      "train loss:0.11274731090968183\n",
      "train loss:0.06830185819419718\n",
      "train loss:0.09446457415646173\n",
      "train loss:0.04069917077789814\n",
      "train loss:0.12142330989157499\n",
      "train loss:0.10828761097251044\n",
      "train loss:0.06684569705314063\n",
      "=== epoch:11, train acc:0.969, test acc:0.945 ===\n",
      "train loss:0.10824073883036851\n",
      "train loss:0.07958051966977622\n",
      "train loss:0.05278462867775979\n",
      "train loss:0.07322976079076052\n",
      "train loss:0.03947696230228872\n",
      "train loss:0.10512642238588844\n",
      "train loss:0.08243119235562396\n",
      "train loss:0.08053853503371894\n",
      "train loss:0.0734812114508667\n",
      "train loss:0.05203480968502094\n",
      "train loss:0.1473631341954174\n",
      "train loss:0.1166089540278204\n",
      "train loss:0.10540010183320263\n",
      "train loss:0.06108140537555684\n",
      "train loss:0.07648859198197369\n",
      "train loss:0.08907235977407252\n",
      "train loss:0.16207434828606102\n",
      "train loss:0.0693310106501884\n",
      "train loss:0.10398285514803572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.03873375823689081\n",
      "train loss:0.08471011650152555\n",
      "train loss:0.088231792528859\n",
      "train loss:0.04329148516981297\n",
      "train loss:0.0699142664331584\n",
      "train loss:0.08933005303668642\n",
      "train loss:0.09206113295630171\n",
      "train loss:0.11490400400517492\n",
      "train loss:0.0891804712204555\n",
      "train loss:0.11071341705578358\n",
      "train loss:0.05249327938604111\n",
      "train loss:0.10463757645258101\n",
      "train loss:0.09259530978959135\n",
      "train loss:0.16971830284049574\n",
      "train loss:0.07848315052949288\n",
      "train loss:0.07321408806626083\n",
      "train loss:0.041734084493194185\n",
      "train loss:0.036249172172032065\n",
      "train loss:0.09986270183656948\n",
      "train loss:0.08778305561483717\n",
      "train loss:0.05519583253622522\n",
      "train loss:0.11846280376211688\n",
      "train loss:0.05579744361612765\n",
      "train loss:0.04198491796245305\n",
      "train loss:0.0766902618053825\n",
      "train loss:0.10324708682597535\n",
      "train loss:0.056897009548524335\n",
      "train loss:0.12373269135952154\n",
      "train loss:0.048621843870814994\n",
      "train loss:0.04140208730354649\n",
      "train loss:0.09575624082168592\n",
      "=== epoch:12, train acc:0.967, test acc:0.944 ===\n",
      "train loss:0.04087542325034055\n",
      "train loss:0.04731668152754161\n",
      "train loss:0.09519061772788234\n",
      "train loss:0.09623830127991226\n",
      "train loss:0.04550022515673633\n",
      "train loss:0.04580996092218915\n",
      "train loss:0.08903074309224986\n",
      "train loss:0.07506407048459135\n",
      "train loss:0.06537996598073499\n",
      "train loss:0.07613384781536245\n",
      "train loss:0.06542363879706505\n",
      "train loss:0.0506251339248801\n",
      "train loss:0.0594016618873309\n",
      "train loss:0.12894945310111364\n",
      "train loss:0.07888313535267374\n",
      "train loss:0.04376055223208719\n",
      "train loss:0.06164356077896775\n",
      "train loss:0.08242996580723747\n",
      "train loss:0.0513342461719082\n",
      "train loss:0.07181215781346417\n",
      "train loss:0.09139665069093308\n",
      "train loss:0.05893176376882523\n",
      "train loss:0.07097284134935263\n",
      "train loss:0.04096664008146709\n",
      "train loss:0.05940093269597036\n",
      "train loss:0.019568015909703366\n",
      "train loss:0.042342687222412634\n",
      "train loss:0.14564503989380853\n",
      "train loss:0.060191607452133845\n",
      "train loss:0.04165579041965007\n",
      "train loss:0.057002051941144494\n",
      "train loss:0.08435027789465806\n",
      "train loss:0.06418343086402252\n",
      "train loss:0.07270155341391775\n",
      "train loss:0.13284907185746578\n",
      "train loss:0.041990482074345525\n",
      "train loss:0.04117084141640702\n",
      "train loss:0.09075809709183191\n",
      "train loss:0.041206577112493224\n",
      "train loss:0.05841898661841947\n",
      "train loss:0.037052253882936384\n",
      "train loss:0.02940394926084649\n",
      "train loss:0.07929359492949431\n",
      "train loss:0.09513125402931528\n",
      "train loss:0.03286909814043\n",
      "train loss:0.10611638257830298\n",
      "train loss:0.048846525101159916\n",
      "train loss:0.02150809454305547\n",
      "train loss:0.10341038699499734\n",
      "train loss:0.05134012094731693\n",
      "=== epoch:13, train acc:0.977, test acc:0.948 ===\n",
      "train loss:0.03142342477664468\n",
      "train loss:0.08007900201318556\n",
      "train loss:0.07452897850743118\n",
      "train loss:0.13043175820121578\n",
      "train loss:0.06798645921204537\n",
      "train loss:0.045846383827719855\n",
      "train loss:0.048987262005574544\n",
      "train loss:0.06939320747704965\n",
      "train loss:0.0316371129578758\n",
      "train loss:0.11174023323352693\n",
      "train loss:0.033960419147700725\n",
      "train loss:0.09338069651640639\n",
      "train loss:0.09023319475130466\n",
      "train loss:0.05166032820863304\n",
      "train loss:0.02351937926334215\n",
      "train loss:0.03431767690373086\n",
      "train loss:0.04616222826877567\n",
      "train loss:0.03856921810222241\n",
      "train loss:0.07732719912687144\n",
      "train loss:0.07174638501365323\n",
      "train loss:0.050103735001192466\n",
      "train loss:0.06616491994422657\n",
      "train loss:0.06948395300001602\n",
      "train loss:0.10615113549524402\n",
      "train loss:0.136792088479005\n",
      "train loss:0.06491347888228371\n",
      "train loss:0.049914405935262786\n",
      "train loss:0.0685137849417738\n",
      "train loss:0.07881578950380443\n",
      "train loss:0.026719212469459998\n",
      "train loss:0.1424468453077578\n",
      "train loss:0.08061356478947075\n",
      "train loss:0.13907819763843465\n",
      "train loss:0.08682155319912198\n",
      "train loss:0.05055786603241838\n",
      "train loss:0.14298024137130688\n",
      "train loss:0.04964460696867326\n",
      "train loss:0.0427692261796372\n",
      "train loss:0.052634366075279715\n",
      "train loss:0.022687725591410645\n",
      "train loss:0.02931510119308036\n",
      "train loss:0.04847874569848632\n",
      "train loss:0.10306302902244308\n",
      "train loss:0.09552592465864151\n",
      "train loss:0.052129522090412196\n",
      "train loss:0.05080216699704805\n",
      "train loss:0.0446493822220357\n",
      "train loss:0.039813750843466424\n",
      "train loss:0.051159584222965154\n",
      "train loss:0.049321126894485735\n",
      "=== epoch:14, train acc:0.978, test acc:0.949 ===\n",
      "train loss:0.06604042115138263\n",
      "train loss:0.041390481354132064\n",
      "train loss:0.06385110355398785\n",
      "train loss:0.048946496217377226\n",
      "train loss:0.03292739191383234\n",
      "train loss:0.03707498690350655\n",
      "train loss:0.03485515186983699\n",
      "train loss:0.09944447551125107\n",
      "train loss:0.07566528085856916\n",
      "train loss:0.016070749843520648\n",
      "train loss:0.06521440172138225\n",
      "train loss:0.05770417177066256\n",
      "train loss:0.04774925636181313\n",
      "train loss:0.03846578730063111\n",
      "train loss:0.13850627505518637\n",
      "train loss:0.07749784924687474\n",
      "train loss:0.06655345093751844\n",
      "train loss:0.03201427542586024\n",
      "train loss:0.05566931697278641\n",
      "train loss:0.02830193050540653\n",
      "train loss:0.02717752216051588\n",
      "train loss:0.10199399479519083\n",
      "train loss:0.04117094300886617\n",
      "train loss:0.05289861897099052\n",
      "train loss:0.050516124539249974\n",
      "train loss:0.03345400522808717\n",
      "train loss:0.13032294047027013\n",
      "train loss:0.054028062011491\n",
      "train loss:0.07872669740901615\n",
      "train loss:0.08377594323994629\n",
      "train loss:0.06285341020851264\n",
      "train loss:0.031411244618859156\n",
      "train loss:0.01952859951592665\n",
      "train loss:0.028699726221256085\n",
      "train loss:0.02467815375776356\n",
      "train loss:0.08256287749469848\n",
      "train loss:0.033684328780602166\n",
      "train loss:0.03715836622018001\n",
      "train loss:0.04428264039992974\n",
      "train loss:0.07902107308147362\n",
      "train loss:0.024064560038365424\n",
      "train loss:0.0375203309309971\n",
      "train loss:0.03351207689580734\n",
      "train loss:0.06862516745017444\n",
      "train loss:0.05664579290559848\n",
      "train loss:0.10704671626789536\n",
      "train loss:0.07569644435301796\n",
      "train loss:0.03832549300305862\n",
      "train loss:0.02911132333713518\n",
      "train loss:0.04948874676620092\n",
      "=== epoch:15, train acc:0.976, test acc:0.948 ===\n",
      "train loss:0.03215001087030339\n",
      "train loss:0.06698921447995328\n",
      "train loss:0.03711928626833472\n",
      "train loss:0.04259422323299524\n",
      "train loss:0.09603978457011791\n",
      "train loss:0.05573733763460816\n",
      "train loss:0.09774112722139401\n",
      "train loss:0.11028292288750027\n",
      "train loss:0.09817410392083817\n",
      "train loss:0.031495961370362285\n",
      "train loss:0.0717353232496424\n",
      "train loss:0.17260519485747952\n",
      "train loss:0.052967850492153606\n",
      "train loss:0.05738250144017451\n",
      "train loss:0.04811308892519126\n",
      "train loss:0.060819775937531934\n",
      "train loss:0.03843860357953875\n",
      "train loss:0.0289487650463107\n",
      "train loss:0.0756877256072366\n",
      "train loss:0.03573885674292858\n",
      "train loss:0.037925732001839414\n",
      "train loss:0.04590645375837247\n",
      "train loss:0.0326562391534138\n",
      "train loss:0.07628630627313356\n",
      "train loss:0.02768440110265185\n",
      "train loss:0.059979507032204823\n",
      "train loss:0.04938435005942425\n",
      "train loss:0.0656804833833562\n",
      "train loss:0.0750462286765087\n",
      "train loss:0.04065449788274487\n",
      "train loss:0.12775657856053504\n",
      "train loss:0.027213681697219563\n",
      "train loss:0.02477686390144628\n",
      "train loss:0.048899710834770724\n",
      "train loss:0.07346819025581874\n",
      "train loss:0.05990603858337236\n",
      "train loss:0.08162639302435226\n",
      "train loss:0.06520054069980509\n",
      "train loss:0.09387264889981621\n",
      "train loss:0.032568701046326455\n",
      "train loss:0.04883122215558683\n",
      "train loss:0.04910230031105951\n",
      "train loss:0.01953574049220715\n",
      "train loss:0.04367387925428707\n",
      "train loss:0.06935048325197445\n",
      "train loss:0.04470443074160874\n",
      "train loss:0.03415269919363823\n",
      "train loss:0.026313417293744083\n",
      "train loss:0.04989966741750425\n",
      "train loss:0.02879360293452227\n",
      "=== epoch:16, train acc:0.974, test acc:0.957 ===\n",
      "train loss:0.0429720837592872\n",
      "train loss:0.05248802314417025\n",
      "train loss:0.05640741791502531\n",
      "train loss:0.024367242593591052\n",
      "train loss:0.020074813032967778\n",
      "train loss:0.04037966223134966\n",
      "train loss:0.042697868345711903\n",
      "train loss:0.022568330659859442\n",
      "train loss:0.019719092640735486\n",
      "train loss:0.15912880330730197\n",
      "train loss:0.11008188546954548\n",
      "train loss:0.058543392347159474\n",
      "train loss:0.05123676678808306\n",
      "train loss:0.04024610016824723\n",
      "train loss:0.01786540896626608\n",
      "train loss:0.031139912397581612\n",
      "train loss:0.032127398656854704\n",
      "train loss:0.04740283182966982\n",
      "train loss:0.047203749450369405\n",
      "train loss:0.01259252731243447\n",
      "train loss:0.04144330477289335\n",
      "train loss:0.038634658307493436\n",
      "train loss:0.02454120294784336\n",
      "train loss:0.04427259593202794\n",
      "train loss:0.02895078741074054\n",
      "train loss:0.039955960379043504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.05727911251827808\n",
      "train loss:0.00842888499415623\n",
      "train loss:0.018098783884147727\n",
      "train loss:0.05946325857693997\n",
      "train loss:0.037111063769020874\n",
      "train loss:0.11306724944460267\n",
      "train loss:0.015276437503579243\n",
      "train loss:0.09328652901125936\n",
      "train loss:0.05529109953085202\n",
      "train loss:0.05281768491877504\n",
      "train loss:0.028922494330090608\n",
      "train loss:0.03599934778452821\n",
      "train loss:0.020328143529799222\n",
      "train loss:0.06611906623674052\n",
      "train loss:0.03528265509146831\n",
      "train loss:0.024029094702741984\n",
      "train loss:0.02579210742277503\n",
      "train loss:0.06561748786534495\n",
      "train loss:0.08016316206972181\n",
      "train loss:0.02723374943076094\n",
      "train loss:0.010813032791187728\n",
      "train loss:0.020442418967922947\n",
      "train loss:0.02849351770473756\n",
      "train loss:0.04447718147650312\n",
      "=== epoch:17, train acc:0.985, test acc:0.951 ===\n",
      "train loss:0.021779484555274564\n",
      "train loss:0.025249811566341766\n",
      "train loss:0.09110823979823778\n",
      "train loss:0.0301622627939218\n",
      "train loss:0.04997618475021093\n",
      "train loss:0.029493224564893714\n",
      "train loss:0.04546538245928908\n",
      "train loss:0.03429891571205634\n",
      "train loss:0.02245382766878131\n",
      "train loss:0.052477518220881106\n",
      "train loss:0.0729052730497167\n",
      "train loss:0.01439295828481593\n",
      "train loss:0.04158767915269063\n",
      "train loss:0.03550958162817437\n",
      "train loss:0.04008884783120271\n",
      "train loss:0.04130932440957217\n",
      "train loss:0.029290690975732826\n",
      "train loss:0.08127691484548252\n",
      "train loss:0.04221029920346015\n",
      "train loss:0.02818488482077733\n",
      "train loss:0.01888980457245586\n",
      "train loss:0.03906746847189625\n",
      "train loss:0.05813272304048699\n",
      "train loss:0.03019150710333842\n",
      "train loss:0.04788400315640037\n",
      "train loss:0.04125386378594131\n",
      "train loss:0.05029750808225278\n",
      "train loss:0.029818354565020706\n",
      "train loss:0.02944484768305788\n",
      "train loss:0.03735610755034005\n",
      "train loss:0.024588423812696034\n",
      "train loss:0.045793087431247555\n",
      "train loss:0.025523710547046878\n",
      "train loss:0.028525078541851167\n",
      "train loss:0.012845546854659405\n",
      "train loss:0.060412436322288085\n",
      "train loss:0.02271435354092298\n",
      "train loss:0.037246545034800786\n",
      "train loss:0.04228838772908709\n",
      "train loss:0.06101548823090381\n",
      "train loss:0.06701628223941856\n",
      "train loss:0.01713068568992269\n",
      "train loss:0.04174125582469276\n",
      "train loss:0.023510652952546\n",
      "train loss:0.07089933233585571\n",
      "train loss:0.009578032441239094\n",
      "train loss:0.02037469833540898\n",
      "train loss:0.06879130960130922\n",
      "train loss:0.02134461362953058\n",
      "train loss:0.03731620769083889\n",
      "=== epoch:18, train acc:0.989, test acc:0.953 ===\n",
      "train loss:0.05267619397237491\n",
      "train loss:0.025517968327994866\n",
      "train loss:0.0373154477370608\n",
      "train loss:0.012815925779542624\n",
      "train loss:0.06459064881461087\n",
      "train loss:0.025208770013489555\n",
      "train loss:0.0391912780523981\n",
      "train loss:0.020741537311242307\n",
      "train loss:0.024923093016647805\n",
      "train loss:0.01340567056251148\n",
      "train loss:0.018356746952853872\n",
      "train loss:0.02383528024730717\n",
      "train loss:0.027390347124515286\n",
      "train loss:0.015764726107363892\n",
      "train loss:0.040947686365656155\n",
      "train loss:0.012073298579647626\n",
      "train loss:0.040929323594139075\n",
      "train loss:0.05134354459596919\n",
      "train loss:0.032826529436550136\n",
      "train loss:0.04213761892039525\n",
      "train loss:0.02367745966150979\n",
      "train loss:0.015374476649723814\n",
      "train loss:0.042386460048355126\n",
      "train loss:0.012848507199311552\n",
      "train loss:0.040758579041727375\n",
      "train loss:0.09891751382948374\n",
      "train loss:0.032263255162182626\n",
      "train loss:0.03132771459476091\n",
      "train loss:0.025422304909146917\n",
      "train loss:0.013985100076217869\n",
      "train loss:0.03724046004286042\n",
      "train loss:0.06657531709156066\n",
      "train loss:0.023986572163095618\n",
      "train loss:0.047118359680554084\n",
      "train loss:0.036599880337829196\n",
      "train loss:0.02690588002114965\n",
      "train loss:0.03240065534129027\n",
      "train loss:0.01993247300895428\n",
      "train loss:0.03034285202210774\n",
      "train loss:0.05544099365235233\n",
      "train loss:0.05105159463885268\n",
      "train loss:0.014431359069470424\n",
      "train loss:0.035038050983308595\n",
      "train loss:0.018458766826443273\n",
      "train loss:0.016914580870828845\n",
      "train loss:0.02771367083580483\n",
      "train loss:0.02002430104551062\n",
      "train loss:0.015425610098292361\n",
      "train loss:0.05888143382042585\n",
      "train loss:0.02566462433683618\n",
      "=== epoch:19, train acc:0.991, test acc:0.957 ===\n",
      "train loss:0.022852803084596987\n",
      "train loss:0.04890189604391065\n",
      "train loss:0.010035250026471022\n",
      "train loss:0.04492711063240058\n",
      "train loss:0.04290382139271939\n",
      "train loss:0.03448034721310293\n",
      "train loss:0.007648011609037928\n",
      "train loss:0.022367627226571485\n",
      "train loss:0.02801359178209941\n",
      "train loss:0.013547620701331333\n",
      "train loss:0.023406056947602213\n",
      "train loss:0.021803357668978774\n",
      "train loss:0.05450810833123235\n",
      "train loss:0.012012801598193712\n",
      "train loss:0.017727884380557862\n",
      "train loss:0.013722680657447724\n",
      "train loss:0.01701712432639069\n",
      "train loss:0.04811735824544116\n",
      "train loss:0.008645824879470673\n",
      "train loss:0.024610629597843298\n",
      "train loss:0.01381447605490389\n",
      "train loss:0.02096458043148726\n",
      "train loss:0.007387568127793357\n",
      "train loss:0.010073280512619662\n",
      "train loss:0.0175885317364828\n",
      "train loss:0.0616057039309268\n",
      "train loss:0.0103274329982671\n",
      "train loss:0.05704119942065001\n",
      "train loss:0.015018418484134134\n",
      "train loss:0.08170840102736349\n",
      "train loss:0.07433378583318273\n",
      "train loss:0.05874207516408649\n",
      "train loss:0.01412770627049975\n",
      "train loss:0.03481238085834666\n",
      "train loss:0.03501893451293152\n",
      "train loss:0.04359505709938842\n",
      "train loss:0.03613340121563188\n",
      "train loss:0.02765795620917281\n",
      "train loss:0.05789410965821479\n",
      "train loss:0.010017012177405138\n",
      "train loss:0.011240295152879245\n",
      "train loss:0.009975976546739254\n",
      "train loss:0.02112390343265742\n",
      "train loss:0.033671669669381045\n",
      "train loss:0.09739994211210902\n",
      "train loss:0.00708229469208813\n",
      "train loss:0.016357793198147102\n",
      "train loss:0.008920876685895671\n",
      "train loss:0.02716042142439309\n",
      "train loss:0.010743039508849335\n",
      "=== epoch:20, train acc:0.995, test acc:0.961 ===\n",
      "train loss:0.018562737576968115\n",
      "train loss:0.006390684561159844\n",
      "train loss:0.013772968710772139\n",
      "train loss:0.012681930754365746\n",
      "train loss:0.02962874742612949\n",
      "train loss:0.01999222347906402\n",
      "train loss:0.006642788638219654\n",
      "train loss:0.03097900107034982\n",
      "train loss:0.027506720890566647\n",
      "train loss:0.026984890479382875\n",
      "train loss:0.013246234849934117\n",
      "train loss:0.022617143605989987\n",
      "train loss:0.024298880943621627\n",
      "train loss:0.027633517621838806\n",
      "train loss:0.010779928503993523\n",
      "train loss:0.014949458548477583\n",
      "train loss:0.037788766492224256\n",
      "train loss:0.014724324961421502\n",
      "train loss:0.018104953390031486\n",
      "train loss:0.05454656907255111\n",
      "train loss:0.009696442374543866\n",
      "train loss:0.020466053240000077\n",
      "train loss:0.034238596655335164\n",
      "train loss:0.01560823008081878\n",
      "train loss:0.008572551924857673\n",
      "train loss:0.04276696880421639\n",
      "train loss:0.01913607260621354\n",
      "train loss:0.03125822998474982\n",
      "train loss:0.023831036285981478\n",
      "train loss:0.024500124406578557\n",
      "train loss:0.016884599305357892\n",
      "train loss:0.01232390793198421\n",
      "train loss:0.030408118471643063\n",
      "train loss:0.01237437325716293\n",
      "train loss:0.048909215579965026\n",
      "train loss:0.010151053594256916\n",
      "train loss:0.008117348504019132\n",
      "train loss:0.06828348289945908\n",
      "train loss:0.008416187143331583\n",
      "train loss:0.01944828962714841\n",
      "train loss:0.03656790423662544\n",
      "train loss:0.026014542562827293\n",
      "train loss:0.02082202602268503\n",
      "train loss:0.049631274807661725\n",
      "train loss:0.017157505019202858\n",
      "train loss:0.014546738772254889\n",
      "train loss:0.05597810207549881\n",
      "train loss:0.008820680145535467\n",
      "train loss:0.012650124267479395\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.956\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_convet.py\n",
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b0a4a6",
   "metadata": {},
   "source": [
    "- =============== Final Test Accuracy ===============\n",
    "- test acc:0.956\n",
    "- Saved Network Parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcd8e75",
   "metadata": {},
   "source": [
    "## 7.6 CNN 시각화하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2041d45e",
   "metadata": {},
   "source": [
    "- 합성곱 계층은 입력으로 받은 이미지 데이터에서 '무엇을 보고 있는' 걸까?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a449d14",
   "metadata": {},
   "source": [
    "### 7.6.1 1번째 층의 가중치 시각화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05a3c862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from simple_convnet import SimpleConvNet\n",
    "\n",
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "network = SimpleConvNet()\n",
    "# 무작위(랜덤) 초기화 후의 가중치\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 학습된 가중치\n",
    "network.load_params(\"params.pkl\")\n",
    "filter_show(network.params['W1'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65448b4",
   "metadata": {},
   "source": [
    "- 학습 건과 후의 1번째 층의 합성곱 계층의 가중치: 가중치의 원소는 실수이지만, 이미지에서는 가장 작은 값(0)은 검은색, 가장 큰 값(255)은 흰색으로 정규화하여 표시함\n",
    "    - 학습 전 -> 후: 규칙성 있는 이미지\n",
    "    - 규칙성 있는 필터는 '무엇을 보고 있는'걸까? \n",
    "    <br/>**-> 엣지(색상이 바뀐 경계선)와 블롭(국소적으로 덩어리진 영역)**\n",
    "\n",
    "<img src = \"../deep_learning_images/fig 7-24.png\" width = \"60%\" height = \"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d28b9a",
   "metadata": {},
   "source": [
    "- 가로 에지와 세로 에지에 반응하는 필터\n",
    "     - 출력 이미지 1: 세로 엣지에 흰 픽셀 많음 \n",
    "     - 출력 이미지 2: 가로 에지에 흰 픽셀 많음\n",
    "- 합성곱 계층의 필터는 엣지나 블롭 등의 원시적인 정보를 추출 가능 -> 뒷단 계층으로 전달\n",
    "     \n",
    "<img src = \"../deep_learning_images/fig 7-25.png\" width = \"60%\" height = \"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36abd878",
   "metadata": {},
   "source": [
    "### 7.6.2 층 깊이에 따른 추출 정보 변화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1acbea",
   "metadata": {},
   "source": [
    "- 1번째 층의 합성곱 계층: 엣지나 블록 등의 저수준 정보\n",
    "- 겹겹이 쌓인 CNN의 각 계층에서는 어떤 정보 추출?\n",
    "    - 계층이 깊어질수록 추출되는 정보(정확히는 강하게 반응하는 뉴런)는 더 '추상화'된다.\n",
    "    \n",
    "- CNN의 합성곱 계층에서 추출되는 정보\n",
    "    - 1번째 층: 엣지와 블록\n",
    "    - 3번째 층: 텍스처\n",
    "    - 5번째 층: 사물의 일부\n",
    "    - 마지막(완전연결) 계층: 사물의 클래스(개, 자동차 등)에 뉴런 반응\n",
    "    \n",
    "    <img src = \"../deep_learning_images/fig 7-26.png\" width = \"60%\" height = \"60%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d43f62",
   "metadata": {},
   "source": [
    "> 층이 깊어지면서 뉴런이 반응하는 대상이 단순한 모양에서 '고급'정보로 변화해간다. 다시 말하면 사물의 '의미'를 이해하도록 변화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b07c1b",
   "metadata": {},
   "source": [
    "## 7.7 대표적인 CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc4230",
   "metadata": {},
   "source": [
    "- LeNet: CNN의 원조\n",
    "- AlexNet: 딥러닝이 주목받도록 이끔"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e410a8",
   "metadata": {},
   "source": [
    "### 7.7.1 LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed225e20",
   "metadata": {},
   "source": [
    "- **LeNet:** 손글씨 숫자를 인식하는 네트워크, 1998년 제안\n",
    "- 시그모이드 함수 사용, 서브 샘플링을 하여 중간 데이터의 크기를 줄임\n",
    "\n",
    "<img src = \"../deep_learning_images/fig 7-27.png\" width = \"60%\" height = \"60%\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda8bca5",
   "metadata": {},
   "source": [
    "### 7.7.2 AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4399905a",
   "metadata": {},
   "source": [
    "- **AlexNet:** 구성은 기본적으로 LeNet과 크게 다르지 않다.\n",
    "\n",
    "<img src = \"../deep_learning_images/fig 7-28.png\" width = \"60%\" height = \"60%\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ea751b",
   "metadata": {},
   "source": [
    "- 다음과 같은 변화\n",
    "    - 활성화 함수로 ReLU를 이용\n",
    "    - LFN(Local Response Normalization)이라는 국소적 정규화를 실시하는 계층을 이용\n",
    "    - 드롭아웃 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02635ffb",
   "metadata": {},
   "source": [
    "## 7.8 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735a14d",
   "metadata": {},
   "source": [
    "- CNN을 구성하는 기본 모듈\n",
    "    - '합성곱 계층'\n",
    "    - '풀링 계층'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66017cb8",
   "metadata": {},
   "source": [
    "> **이번 장에서 배운 내용**\n",
    "* CNN은 지금까지의 완전연결 계층 네트워크에 합성곱 계층과 풀링 계층을 새로 추가한다.\n",
    "* 합성곱 계층과 풀링 계층은 im2col (이미지를 행렬로 전개하는 함수)을 이용하면 간단하고 효율적으로 구현할 수 있다.\n",
    "* CNN을 시각화해보면 계층이 깊어질수록 고급 정보가 추출되는 모습을 확인할 수 있다.\n",
    "* 대표적인 CNN에는 LeNet과 AlexNet이 있다.\n",
    "* 딥러닝의 발전에는 빅 데이터와 GPU가 크게 기여했다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
