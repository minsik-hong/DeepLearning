{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f84ff1",
   "metadata": {},
   "source": [
    "# 밑바닥 부터 시작하는 딥러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3585a56b",
   "metadata": {},
   "source": [
    "## 목차\n",
    "```\n",
    "5.1 계산 그래프 \n",
    "__5.1.1 계산 그래프로 풀다 \n",
    "__5.1.2 국소적 계산 \n",
    "__5.1.3 왜 계산 그래프로 푸는가? \n",
    "5.2 연쇄법칙 \n",
    "__5.2.1 계산 그래프에서의 역전파 \n",
    "__5.2.2 연쇄법칙이란? \n",
    "__5.2.3 연쇄법칙과 계산 그래프 \n",
    "5.3 역전파 \n",
    "__5.3.1 덧셈 노드의 역전파 \n",
    "__5.3.2 곱셈 노드의 역전파 \n",
    "__5.3.3 사과 쇼핑의 예 \n",
    "5.4 단순한 계층 구현하기 \n",
    "__5.4.1 곱셈 계층 \n",
    "__5.4.2 덧셈 계층 \n",
    "5.5 활성화 함수 계층 구현하기 \n",
    "__5.5.1 ReLU 계층 \n",
    "__5.5.2 Sigmoid 계층 \n",
    "5.6 Affine/Softmax 계층 구현하기 \n",
    "__5.6.1 Affine 계층 \n",
    "__5.6.2 배치용 Affine 계층 \n",
    "__5.6.3 Softmax-with-Loss 계층 \n",
    "5.7 오차역전파법 구현하기 \n",
    "__5.7.1 신경망 학습의 전체 그림 \n",
    "__5.7.2 오차역전파법을 적용한 신경망 구현하기 \n",
    "__5.7.3 오차역전파법으로 구한 기울기 검증하기 \n",
    "__5.7.4 오차역전파법을 사용한 학습 구현하기\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3c582c",
   "metadata": {},
   "source": [
    "# Chapter 5: 오차역전파법\n",
    "\n",
    "- 수치 미분: 단순하고 구현하기 쉽다 but, 오래 걸린다.\n",
    "- **오차역전파법(backpropagation):** 가중치 매개변수의 기울기를 효율적으로 계산 가능\n",
    "    - 계산 그래프를 통한 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f593f19",
   "metadata": {},
   "source": [
    "## 5.1 계산 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb350ce9",
   "metadata": {},
   "source": [
    "- 계산 그래프(computational graph): 노드(node)와 엣지(edge)로 포현 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225bcdb5",
   "metadata": {},
   "source": [
    "### 5.1.1 계산 그래프로 풀다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16071b",
   "metadata": {},
   "source": [
    "- 문제 1 : 현빈 군은 슈퍼에서 1개의 100원인 사과를 2개 샀습니다. 이때 지불 금액을 구하세요. 단, 소비세가 10% 부과됩니다.\n",
    "\n",
    "<img src = \"deep_learning_images/fig 5-1.png\" width = \"50%\" height = \"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887bc97",
   "metadata": {},
   "source": [
    "- 계산 그래프로 풀어본 문제 1의 답: '사과의 개수'와 '소비세'를 변수로 취급해 원 밖에 표기\n",
    "\n",
    "<img src = \"deep_learning_images/fig 5-2.png\" width = \"50%\" height = \"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec61e6d",
   "metadata": {},
   "source": [
    "- 문제 2 : 현빈 군은 슈퍼에서 사과를 2개, 귤을 3개 샀습니다. 사과는 1개에 100원, 귤은 1개 150원입니다. 소비세가 10%일 때 지불 금액을 구하세요.\n",
    "\n",
    "<img src = \"deep_learning_images/fig 5-3.png\" width = \"50%\" height = \"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ecfd1",
   "metadata": {},
   "source": [
    "- 회로에 전류가 흐르듯 계산 결과가 왼쪽에서 오른쪽으로 전달\n",
    "- 계산 그래프 문제풀이 흐름\n",
    "    1. 계산 그래프를 구성한다.\n",
    "    2. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다.(순전파, forward propagation) <-> (역전파, backward propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f52ef3",
   "metadata": {},
   "source": [
    "### 5.1.2 국소적 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27934db1",
   "metadata": {},
   "source": [
    "- 계산 그래프의 특징: '국소적 계산'을 전파함으로써 최종 결과를 얻는다\n",
    "\n",
    "    <img src = \"deep_learning_images/fig 5-4.png\" width = \"50%\" height = \"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95565c0",
   "metadata": {},
   "source": [
    "- 전체 계산이 제아무리 복잡하더라도 각 단계에서 하는 일은 해당 노드의 '국소적 계산'\n",
    "- 국소적 계산은 단순하지만, 그 결과를 전달함으로써 전체를 구성하는 복잡한 계산을 해낼 수 있다.\n",
    "> 복잡한 계산도 분해하면 단순한 계산으로 구성됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72535fee",
   "metadata": {},
   "source": [
    "### 5.1.3 왜 계산 그래프로 푸는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07a9aa3",
   "metadata": {},
   "source": [
    "- 계산 그래프의 이점\n",
    "    1. 국소적 계산: 전체가 아무리 복잡해도 각 노드에서는 단순한 계산에 집중하여 문제를 단순화 할 수 있다.\n",
    "    2. 중간 계산 결과 보관 가능\n",
    "    3. **역전파를 통해 '미분'을 효율적으로 계산할 수 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14831de0",
   "metadata": {},
   "source": [
    "- 역전파에 의한 미분 값의 전달\n",
    "    \n",
    "    <img src = \"deep_learning_images/fig 5-5.png\" width = \"50%\" height = \"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d4d279",
   "metadata": {},
   "source": [
    "- 오른쪽에서 왼쪽으로 '1 -> 1.1 -> 2.2' 순으로 미분 값 전달: '사과 가격에 대한 지불 금액의 미분'값은 2.2.\n",
    "- 사과가 1원 오르면 최종 금액은 2.2배만큼 오른다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6cc34d",
   "metadata": {},
   "source": [
    "## 5.2 연쇄법칙"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eef056a",
   "metadata": {},
   "source": [
    "- '국소적 미분'을 전달하는 원리: **연쇄법칙(chain rule)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f952b5cc",
   "metadata": {},
   "source": [
    "### 5.2.1 계산 그래프의 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de94970",
   "metadata": {},
   "source": [
    "- y = f(x)의 역전파\n",
    "\n",
    "    <img src = \"deep_learning_images/fig 5-6.png\" width = \"30%\" height = \"30%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82314e9",
   "metadata": {},
   "source": [
    "- 신호E에 국소적 미분을 곱한 후 다음 노드로 전달\n",
    "- ex) f(x)=x^2 -> E * 2x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575aa8e",
   "metadata": {},
   "source": [
    "### 5.2.2 연쇄법칙이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69881c3e",
   "metadata": {},
   "source": [
    "- 연쇄법칙은 합성 함수의 미분에 대한 성질\n",
    "> 함성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a778445",
   "metadata": {},
   "source": [
    "- 합성함수 z = (x+y)^2\n",
    "\n",
    "    <img src = \"deep_learning_images/e 5.1.png\" width = \"20%\" height = \"20%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a338e50",
   "metadata": {},
   "source": [
    "- (x에 대한 z의 미분) = (t에 대한 z의 미분) x (x에 대한 t의 미분)\n",
    "\n",
    "    <img src = \"deep_learning_images/e 5.2.png\" width = \"15%\" height = \"15%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e026f59",
   "metadata": {},
   "source": [
    "- 연쇄법칙을 써서 x에 대한 z의 미분 구하기.\n",
    "- 국소적 미분(편미분) 구하기\n",
    "\n",
    "    <img src = \"deep_learning_images/e 5.3.png\" width = \"10%\" height = \"10%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ee1dc",
   "metadata": {},
   "source": [
    "- 구한 두 미분의 곱\n",
    "\n",
    "    <img src = \"deep_learning_images/e 5.4.png\" width = \"30%\" height = \"30%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb2cac",
   "metadata": {},
   "source": [
    "### 5.2.3 연쇄법칙과 계산 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf65f63f",
   "metadata": {},
   "source": [
    "- 순전파와는 반대 방향으로 국소적 미분을 곱하여 전달\n",
    "\n",
    "\n",
    "<img src = \"deep_learning_images/fig 5-7.png\" width = \"40%\" height = \"40%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330c4b5d",
   "metadata": {},
   "source": [
    "- 맨 왼쪽 역전파: 연쇄법칙에 의해 'x에 대한 z의 미분'. 즉,  역전파가 하는 일은 연쇄법칙의 원리와 같다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d469e",
   "metadata": {},
   "source": [
    "<img src = \"deep_learning_images/fig 5-8.png\" width = \"40%\" height = \"40%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd0b4da",
   "metadata": {},
   "source": [
    "## 5.3 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bad148",
   "metadata": {},
   "source": [
    "### 5.3.1 덧셈 노드의 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ed572c",
   "metadata": {},
   "source": [
    "- ex) z = x + y\n",
    "- 미분\n",
    "\n",
    "    <img src = \"deep_learning_images/e 5.5.png\" width = \"10%\" height = \"10%\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433ede52",
   "metadata": {},
   "source": [
    "- 덧셈 노드의 역전파: 왼쪽이 순전파, 오른쪽이 역전파.\n",
    "- **덧셈 노드의 역전파는 입력 값을 그대로 흘려보낸다.**\n",
    "\n",
    "<img src = \"deep_learning_images/fig 5-9.png\" width = \"50%\" height = \"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082bcd68",
   "metadata": {},
   "source": [
    "- L: 최종 출력 값 가정\n",
    "    \n",
    "<img src = \"deep_learning_images/fig 5-10.png\" width = \"50%\" height = \"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580ec878",
   "metadata": {},
   "source": [
    "- 10 + 5 = 15\n",
    "\n",
    "<img src = \"deep_learning_images/fig 5-11.png\" width = \"50%\" height = \"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2161164",
   "metadata": {},
   "source": [
    "### 5.3.2 곱셈 노드의 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b38fd",
   "metadata": {},
   "source": [
    "- ex) z = xy\n",
    "- 미분\n",
    "\n",
    "    <img src = \"deep_learning_images/e 5.6.png\" width = \"10%\" height = \"10%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0911e4fc",
   "metadata": {},
   "source": [
    "- 곱셈 노드의 역전파: 왼쪽이 순전파, 오른쪽이 역전파.\n",
    "- **(곱셈 노드 역전파) = (상류의 값) x (순전파 때의 입력 신호들을 '서로 바꾼 값')**\n",
    "\n",
    "<img src = \"deep_learning_images/fig 5-12.png\" width = \"50%\" height = \"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23e96bd",
   "metadata": {},
   "source": [
    "- 10 x 5 = 50\n",
    "- 5 x 1.3 = 6.5\n",
    "- 10 x 1.3 = 1.3\n",
    "\n",
    "<img src = \"deep_learning_images/fig 5-13.png\" width = \"50%\" height = \"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62455e7",
   "metadata": {},
   "source": [
    "- 덧셈의 역전파: 상류의 값을 그대로 흘려보내서 순방향 입력 신호의 값은 필요하지 않다.\n",
    "- 곱셈의 역전파: 순방향 입력 신호 값 필요. 변수에 저장."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10744bb6",
   "metadata": {},
   "source": [
    "### 5.3.3 사과 쇼핑의 예"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee6c7dd",
   "metadata": {},
   "source": [
    "- 변수\n",
    "    - 사과의 가격\n",
    "    - 사과의 개수\n",
    "    - 소비세\n",
    "- 최종 금액에 어떻게 영향?\n",
    "    - 사과 가격에 대한 지불 금액의 미분: 2.2\n",
    "    - 사과 개수에 대한 지불 금액의 미분: 110\n",
    "    - 소비세에 대한 지불 금액의 미분: 200\n",
    "    \n",
    "\n",
    "<img src = \"deep_learning_images/fig 5-14.png\" width = \"60%\" height = \"60%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e40834",
   "metadata": {},
   "source": [
    "## 5.4 단순한 계층 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07044208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    # 순전파\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    # 역전파\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y # x와 y를 바꾼다\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a80e0",
   "metadata": {},
   "source": [
    "<img src = \"deep_learning_images/fig 5-16.png\" width = \"60%\" height = \"60%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98cef4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b09b12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "#역전파\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2ba572",
   "metadata": {},
   "source": [
    "### 5.4.2 덧셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9670683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd33e18",
   "metadata": {},
   "source": [
    "<img src = \"deep_learning_images/fig 5-17.png\" width = \"60%\" height = \"60%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "276dd674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num) # (1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num) # (2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price) # (3)\n",
    "price = mul_tax_layer.forward(all_price, tax) # (4)\n",
    "\n",
    "# 역전파\n",
    "dprice = 1\n",
    "dallprice, dtax = mul_tax_layer.backward(dprice) # (4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dallprice) # (3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price) # (2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price) # (1)\n",
    "\n",
    "print(price)\n",
    "print(dapple_num, dapple, dorange, dorange_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d9762d",
   "metadata": {},
   "source": [
    "## 5.5 활성화 함수 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7359414",
   "metadata": {},
   "source": [
    "- 활성화 함수\n",
    "    - ReLU 계층 구현\n",
    "    - Sigmoid 계층 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a411303c",
   "metadata": {},
   "source": [
    "### 5.5.1 ReLU 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e619984",
   "metadata": {},
   "source": [
    "- ReLU 수식\n",
    "\n",
    "\n",
    "<img src = \"deep_learning_images/e 5.7.png\" width = \"30%\" height = \"30%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594992c6",
   "metadata": {},
   "source": [
    "- 미분\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"deep_learning_images/e 5.8.png\" width = \"30%\" height = \"30%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39ff365",
   "metadata": {},
   "source": [
    "<img src = \"deep_learning_images/fig 5-18.png\" width = \"70%\" height = \"70%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "151ecb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    # 넘파이 배열로 인수를 받는다\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    # 넘파이 배열로 인수를 받는다\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d9342a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(x)\n",
    "\n",
    "mask = (x<=0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eda2cc3",
   "metadata": {},
   "source": [
    "### 5.5.2 Sigmoid 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2013fc",
   "metadata": {},
   "source": [
    "<img src = \"deep_learning_images/e 5.9.png\" width = \"30%\" height = \"30%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1d04a",
   "metadata": {},
   "source": [
    "- 순전파 계산 그래프\n",
    "\n",
    "\n",
    "<img src = \"deep_learning_images/fig 5-19.png\" width = \"70%\" height = \"70%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dec0cd",
   "metadata": {},
   "source": [
    "- 역전파 흐름"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac9897",
   "metadata": {},
   "source": [
    "- **1 단계 ('/'노드)**\n",
    "    - y = 1/x 미분\n",
    "    - (상류에서 흘러온 값) x (-y^2, 순전파의 출력을 제곱한 후 마이너스를 붙인 값)\n",
    "\n",
    "    <img src = \"deep_learning_images/e 5.10.png\" width = \"20%\" height = \"20%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76603e08",
   "metadata": {},
   "source": [
    "- **2 단계 ('+'노드)**\n",
    "    - 그대로 하류로"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86667302",
   "metadata": {},
   "source": [
    "- **3 단계 ('exp'노드)**\n",
    "    - y = exp(x) 미분\n",
    "    - (상류에서 흘러온 값) x (순전파 때의 출력)\n",
    "\n",
    "    <img src = \"deep_learning_images/e 5.11.png\" width = \"20%\" height = \"20%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b069cfbd",
   "metadata": {},
   "source": [
    "- **4 단계 ('x'노드)**\n",
    "    - '서로 바꿔' 곱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e02734",
   "metadata": {},
   "source": [
    "- Sigmoid 계층의 계산 그래프\n",
    "\n",
    "<img src = \"deep_learning_images/fig 5-20.png\" width = \"70%\" height = \"70%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b965c5",
   "metadata": {},
   "source": [
    "- 계산 그래프의 중간 과정을 모두 묶어 단순한 'sigmoid' 노드 하나로 대체 가능\n",
    "- 역전파 과정의 중간 계산들을 생략할 수 있어 더 효율적인 계산\n",
    "- Sigmoid 계층의 역전파는 순전파의 출력(y)만으로 계산 가능\n",
    "\n",
    "<img src = \"deep_learning_images/fig 5-21.png\" width = \"60%\" height = \"60%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa82acce",
   "metadata": {},
   "source": [
    "<img src = \"deep_learning_images/e 5.12.png\" width = \"60%\" height = \"60%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c59fab",
   "metadata": {},
   "source": [
    "<img src = \"deep_learning_images/fig 5-22.png\" width = \"60%\" height = \"60%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db06456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1+np.exp(-x))\n",
    "        self.out = out # 순전파의 출력을 인스턴스 변수 out에 보관\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c05289",
   "metadata": {},
   "source": [
    "## 5.6 Affine/Softmax 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af02da8",
   "metadata": {},
   "source": [
    "### 5.6.1 Affine 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa573d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "(3,)\n",
      "[1.69238563 1.40828733 0.79051141]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(2) # 입력\n",
    "W = np.random.rand(2,3) # 가중치\n",
    "B = np.random.rand(3) # 편향\n",
    "\n",
    "print(X.shape) # (2, )\n",
    "print(W.shape) # (2, 3)\n",
    "print(B.shape) # (3, )\n",
    "\n",
    "Y = np.dot(X, W) + B\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3a93a2",
   "metadata": {},
   "source": [
    "- 행렬의 곱 계산은 대응하는 차원의 원소 수를 일치시키는 게 핵심\n",
    "- **어파인 변환(affine transformation):** 신경망의 순전파 때 수행하는 행렬의 곱(기하학)\n",
    "\n",
    "<img src = \"deep_learning_images/fig 5-23.png\" width = \"40%\" height = \"40%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd97ba2e",
   "metadata": {},
   "source": [
    "- Affine 계층의 계산 그래프: 변수가 행렬임에 주의\n",
    "\n",
    "<img src = \"deep_learning_images/fig 5-24.png\" width = \"50%\" height = \"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6304a130",
   "metadata": {},
   "source": [
    "- Affine 계층의 역전파: 변수가 다차원 배열임에 주의\n",
    "- 전치행렬\n",
    "\n",
    "\n",
    "<img src = \"deep_learning_images/fig 5-25.png\" width = \"60%\" height = \"60%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee103813",
   "metadata": {},
   "source": [
    "- 행렬 곱('dot'노드)의 역전파는 행렬의 대응하는 차원의 원소 수가 일치하도록 곱을 조립하여 구할 수 있다.\n",
    "- 형상이 같다는 점 이용\n",
    "\n",
    "\n",
    "<img src = \"deep_learning_images/fig 5-26.png\" width = \"60%\" height = \"60%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb368941",
   "metadata": {},
   "source": [
    "### 5.6.2 배치용 Affine 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338874e4",
   "metadata": {},
   "source": [
    "- 데이터 N개를 묶어 순전파하는 경우\n",
    "- 배치용 Affine 계층의 계산 그래프\n",
    "\n",
    "\n",
    "<img src = \"deep_learning_images/fig 5-27.png\" width = \"60%\" height = \"60%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b7d5c",
   "metadata": {},
   "source": [
    "- 입력인 X의 형상이 (N,2)\n",
    "- 편향을 더할 때 주의필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "178cde36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "# 편향의 덧셈은 각 데이터에 더해진다\n",
    "X_dot_W = np.array([[0,0,0],[10,10,10]])\n",
    "B = np.array([1,2,3])\n",
    "\n",
    "print(X_dot_W)\n",
    "print(X_dot_W + B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9232f978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "# 역전파 때는 각 데이터의 역전파 값이 편향의 원소에 모여야 한다.\n",
    "dY = np.array([[1,2,3],[4,5,6]])\n",
    "print(dY)\n",
    "\n",
    "dB = np.sum(dY, axis=0) # 0번째 축의 총합으로..\n",
    "print(dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "57c72991",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dout(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961950dc",
   "metadata": {},
   "source": [
    "### 5.6.3 Softmax-with-Loss 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293e6d92",
   "metadata": {},
   "source": [
    "- 출력층에서 사용하는 소프트맥스 함수\n",
    "- 소프트맥스 함수는 입력 값을 정규화(출력의 합이 0이 되도록 변형)하여 출력한다.\n",
    "\n",
    "\n",
    "<img src = \"deep_learning_images/fig 5-28.png\" width = \"60%\" height = \"60%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ccf8a2",
   "metadata": {},
   "source": [
    "> 신경망에서 수행하는 작업 **'학습'**, **'추론'**. 추론할 때는 일반적으로 Softmax 계층을 사용하지 않는다. 신경망 추론에서 답을 하나만 내는 경우에는 가장 높은 점수만 알면되기 때문. (Softmax 앞의 Affine 계층의 출력을 **'점수(score)'** 라 한다). 반면, 신경망 학습할 때는 Softmax 계층이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b314835",
   "metadata": {},
   "source": [
    "- 손실함수인 교차 엔트로피 오차도 포함 -> 'Softmax-with-Loss'계층\n",
    "\n",
    "<img src = \"deep_learning_images/fig 5-29.png\" width = \"70%\" height = \"70%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b53db6",
   "metadata": {},
   "source": [
    "- '간소화한'Softmax-with-Loss 계층의 계산 그래프\n",
    "\n",
    "<img src = \"deep_learning_images/fig 5-30.png\" width = \"60%\" height = \"60%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b2c8d",
   "metadata": {},
   "source": [
    "- 3클래스 분류\n",
    "    - Softmax 계층 입력 (a1, a2, a3) -> 정규화 (y1, y2, y3)\n",
    "    - 정답 레이블 (t1, t2, t3) -> 손실 L\n",
    "- 역전파의 결과\n",
    "    - (y1 - t1, y2 - t2, y3 - t3): Softmax 계층의 출력과 정답 레이블의 차분\n",
    "    - 신경망의 현재 출력과 정답 레이블의 오차를 있는 그대로 드러낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f43df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5.2\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c) # 오버플로 대책\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "\n",
    "    return y\n",
    "\n",
    "# 4.2.4\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1: # y가 1차원이라면(데이터 하나당 교차 엔트로피 오차를 구하는 경우)\n",
    "        t = t.reshape(1, t.size) # 데이터 형상 바꿈\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size  # 배치 크기로 나누어 정규화\n",
    "\n",
    "\n",
    "# Softmax-with-Loss 계층 구현\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실\n",
    "        self.y = None # softmax의 출력\n",
    "        self.t = None # 정답 레이블(원-핫 벡터)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1fb489",
   "metadata": {},
   "source": [
    "## 5.7 오차역전파법 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf7eeb6",
   "metadata": {},
   "source": [
    "### 5.7.1 신경망 학습의 전체 그림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577aa666",
   "metadata": {},
   "source": [
    "- 신경망 학습의 절차\n",
    "    - 전제:<br/>신경망에는 적용 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다. 신경망 학습은 다음과 같이 4단계로 수행한다.\n",
    "        \n",
    "    1. 미니배치:<br/> 훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표다.\n",
    "    <br/> **-> 확률적 경사 하강법(stochastic gradient descent, SGD)**\n",
    "    2. 기울기 산출:<br/> 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n",
    "    <br/> **-> 수치미분 / 오차역전파법**\n",
    "    3. 매개변수 갱신:<br/> 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
    "    4. 반복:<br/> 1 ~ 3단계를 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57801b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "# 2층 신경망 클래스 TwoLayerNet\n",
    "class TwoLayerNet:\n",
    "    # 초기화 수행\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        \n",
    "        # 가중치 초기화: 정규분포 난수, 편향은 0으로\n",
    "        self.params = {} # 매개변수 보관 딕셔너리\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict() # 순서가 있는 딕셔너리\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "    \n",
    "    # 예측(추론) 수행\n",
    "    def predict(self, x):\n",
    "        \n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # x: 입력 데이터, t: 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return self.lasyLayer.forward(y, t)\n",
    "    \n",
    "    # 정확도 구하기\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # 가중치 매개변수의 기울기 구하기(수치 미분 방식) / 고속(오차역전파법)\n",
    "    # x: 입력 데이터, t: 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {} # 기울기 보관 딕셔너리 \n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # 순전파\n",
    "        self.loss(x, t) \n",
    "        \n",
    "        # 역전파\n",
    "        dout = 1 \n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse() # 역순으로 호출\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d8addf",
   "metadata": {},
   "source": [
    "### 5.7.3 오차역전파법으로 구한 기울기 검증하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395432bc",
   "metadata": {},
   "source": [
    "- 기울기를 구현하는 두 가지 방법 \n",
    "    1. 수치 미분: 느리다 / 간단, 버그 가능성 낮음\n",
    "    2. 오차역전파법(해석적): 효율적 / 복잡, 버그 가능성 있음\n",
    "    <br/>**-> 기울기 확인(gradient check):** 수치 미분을 통해 오차역전파법 결과를 검증\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e29d3aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:4.689236677889815e-10\n",
      "b1:2.915377738366663e-09\n",
      "W2:5.115742907919787e-09\n",
      "b2:1.4018969399648417e-07\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  \n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch) # 수치미분\n",
    "grad_backprop = network.gradient(x_batch, t_batch) # 오차역전파법\n",
    "\n",
    "# 각 가중치의 차이의 절댓값을 구한 후, 그 절댓값들의 평균을 낸다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) ) \n",
    "    print(key + \":\" + str(diff))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35736091",
   "metadata": {},
   "source": [
    "### 5.7.4 오차역전파법을 사용한 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a4f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  \n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000 # 반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "# 에폭 리스트 추가\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수(10,000 / 100 -> 100회)\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    \n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch) # 성능 개선판!, 오차역전파법으로 기울기를 구한다.\n",
    "\n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0: # 100번 마다(1에폭 마다)\n",
    "        \n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        \n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        \n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
