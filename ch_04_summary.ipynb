{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0684883c",
      "metadata": {},
      "source": [
        "# 밑바닥 부터 시작하는 딥러닝"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55b3ad1e",
      "metadata": {},
      "source": [
        "## 목차\n",
        "```\n",
        "4.1 데이터에서 학습한다! \n",
        "__4.1.1 데이터 주도 학습 \n",
        "__4.1.2 훈련 데이터와 시험 데이터 \n",
        "4.2 손실 함수 \n",
        "__4.2.1 평균 제곱 오차 \n",
        "__4.2.2 교차 엔트로피 오차 \n",
        "__4.2.3 미니배치 학습 \n",
        "__4.2.4 (배치용) 교차 엔트로피 오차 구현하기 \n",
        "__4.2.5 왜 손실 함수를 설정하는가? \n",
        "4.3 수치 미분 \n",
        "__4.3.1 미분 \n",
        "__4.3.2 수치 미분의 예 \n",
        "__4.3.3 편미분 \n",
        "4.4 기울기 \n",
        "__4.4.1 경사법(경사 하강법) \n",
        "__4.4.2 신경망에서의 기울기 \n",
        "4.5 학습 알고리즘 구현하기 \n",
        "__4.5.1 2층 신경망 클래스 구현하기\n",
        "__4.5.2 미니배치 학습 구현하기 \n",
        "__4.5.3 시험 데이터로 평가하기\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bd6d7e4",
      "metadata": {},
      "source": [
        "# Chapter 4: 신경망 학습\n",
        "\n",
        "- **학습:** 훈련 데이터로부터 가중치 매개변수의 최적값을 자동을 획득하는 것\n",
        "- 학습 지표: 손실 함수(결과값을 가장 작게 만드는 가중치 매개변수를 찾는 것이 목표)\n",
        "- 손실 함수의 값을 가급적 작게 만드는 기법 → 함수의 기울기를 활용하는 경사법"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65a53688",
      "metadata": {},
      "source": [
        "## 4.1 데이터에서 학습한다!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86e4f8c3",
      "metadata": {},
      "source": [
        "- 데이터를 보고 학습 == 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다\n",
        "- 신경망 학습 -> 데이터로부터 매개변수의 값을 정하는 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2630d23",
      "metadata": {},
      "source": [
        "### 4.1.1 데이터 주도 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b207166",
      "metadata": {},
      "source": [
        "- 기계학습: 사람의 개입 최소화, 수집한 **데이터**로부터 패턴을 찾으려 시도\n",
        "    - 이미지 데이터 → 벡터 → 분류기법을 활용해 학습\n",
        "    - 모아진 데이터로 규칙을 찾아내는 역할: '기계' but 이미지를 벡터로 변환할 때 사용하는 **특징(feature)**은 여전히 ‘사람’이 설계\n",
        "- 신경망: 이미지를 ‘있는 그대로’ 학습, end-to-end machine learning\n",
        "    - 이미지에 포함된 중요한 특징까지도 ‘기계’가 스스로 학습\n",
        "    - 이점: 모든 문제를 같은 맥락에서 풀 수 있다"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e17f20e9",
      "metadata": {},
      "source": [
        "- 규칙을 '사람'이 만드는 방식에서 '기계'가 데이터로부터 배우는 방식으로 패러다임 전환: 회색 블록은 사람이 개입하지 않음을 뜻한다.\n",
        "\n",
        "    <img src = \"deep_learning_images/fig 4-2.png\" width = \"50%\" height = \"50%\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38e8afad",
      "metadata": {},
      "source": [
        "### 4.1.2 훈련 데이터와 시험 데이터"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8910d8a",
      "metadata": {},
      "source": [
        "- 훈련 데이터만 사용하여 학습 → 최적의 매개변수 찾기 → 시험 데이터를 사용하여 앞서 훈련한 모델의 실력 평가\n",
        "- **범용능력:** 아직 보지 못한 데이터(훈련 데이터에 포함되지 않는 데이터)로도 문제를 올바르게 풀어내는 능력(최종목표)\n",
        "- **오버피팅(overfitting):** 한 데이터셋에만 지나치게 최적화된 상태(과적합, 과대적합, 과적응), 편견이 생긴 상태"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037415f8",
      "metadata": {},
      "source": [
        "## 4.2 손실 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad430e66",
      "metadata": {},
      "source": [
        "- 신경망 학습: 현재의 상태를 ‘하나의 지표’로 표현\n",
        "- ‘하나의 지표’를 기준으로 최적의 매개변수 값을 탐색\n",
        "- **지표 → 손실함수(loss function):**\n",
        "    - 오차제곱합(Sum of Squares for Error, SSE)\n",
        "    - 교차 엔트로피 오차(Cross Entropy Error, CEE) 사용\n",
        "    \n",
        "> '손실 함수'는 신경망 성능의 '나쁨'을 나타내는 지표, '나쁨을 최소로 하는 것'과 '좋음을 최대로 하는 것'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eecdefe",
      "metadata": {},
      "source": [
        "### 4.2.1 오차제곱합"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "372999f1",
      "metadata": {},
      "source": [
        "- 오차제곱합(Sum of Squares for Error, SSE): 가장 많이 쓰이는 손실 함수\n",
        "\n",
        "    <img src = \"deep_learning_images/e 4.1.png\" width = \"30%\" height = \"30%\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "064771f5",
      "metadata": {},
      "source": [
        "- y_k: 신경망의 출력(신경망이 추정한 값)\n",
        "- t_k: 정답 레이블\n",
        "- k: 데이터의 차원 수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "36c5117e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# 원소 10개 짜리\n",
        "# 첫 번째 인덱스 부터 '0', '1', '2' ...\n",
        "\n",
        "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # 소프트맥스 함수의 출력 -> 확률로 해석\n",
        "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # 원-핫 인코딩(정답 '2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0af297cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sum_squares_error(y, t):\n",
        "    return 0.5 * np.sum((y-t)**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8a1601f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # 정답 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "911003e2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.09750000000000003\n"
          ]
        }
      ],
      "source": [
        "# 예1: 2일 확률이 가장 높다고 추정(0.6)\n",
        "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] \n",
        "print(sum_squares_error(np.array(y), np.array(t))) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "60990f8e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5975\n"
          ]
        }
      ],
      "source": [
        "# 예2: 7일 확률이 가장 높다(0.6)\n",
        "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0] \n",
        "print(sum_squares_error(np.array(y), np.array(t))) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "371f2cbe",
      "metadata": {},
      "source": [
        "- 예1의 손실 함수 쪽 출력이 작다 -> 정답 레이블과의 오차도 작다\n",
        "- 즉, 오차제곱합 기준으로는 첫 번째 추정 결과가(오차가 더 작으니) 정답에 더 가까울 것으로 판단"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8380cd60",
      "metadata": {},
      "source": [
        "### 4.2.2 교차 엔트로피 오차"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fcd89a7",
      "metadata": {},
      "source": [
        "- 오차제곱합(Sum of Squares for Error, SSE): 가장 많이 쓰이는 손실 함수\n",
        "\n",
        "    <img src = \"deep_learning_images/e 4.2.png\" width = \"30%\" height = \"30%\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eab84be",
      "metadata": {},
      "source": [
        "- y_k: 신경망의 출력\n",
        "- t_k: 정답 레이블(원-핫 인코딩)\n",
        "- 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 된다\n",
        "    - 정답 레이블 '2', 신경망 출력 0.6 -> 교차 엔트로피 오차: -log0.6 = 0.51\n",
        "    - 정답 레이블 '2', 신경망 출력 0.1 -> 교차 엔트로피 오차: -log0.1 = 2.30\n",
        "    - 정답 레이블 '2', 신경망 출력 1.0 -> 교차 엔트로피 오차: -log1.0 = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e44912",
      "metadata": {},
      "source": [
        "<img src = \"deep_learning_images/fig 4-3.png\" width = \"40%\" height = \"40%\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca784913",
      "metadata": {},
      "source": [
        "- 자연로그 그래프\n",
        "    - x=1 -> y=0\n",
        "    - x가 0에 가까워질수록 y의 값은 점점 작아진다\n",
        "- 교차 엔트로피 오차 식\n",
        "    - 정답에 해당하는 출력이 커질수록 0에 다가가다가, 그 출력이 1일 때 0이 된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fcb99876",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_entropy_error(y, t):\n",
        "    delta = 1e-7 # 0이 되지 않도록 아주 작은 값(np.log()가 0이면 마이너스 무한대)\n",
        "    return -np.sum(t * np.log(y + delta))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bb8bf699",
      "metadata": {},
      "outputs": [],
      "source": [
        "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # 정답 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5fb0e774",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.510825457099338\n"
          ]
        }
      ],
      "source": [
        "# 예1: 2일 확률이 가장 높다고 추정(0.6)\n",
        "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] \n",
        "print(cross_entropy_error(np.array(y), np.array(t))) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "12bdfe9d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.302584092994546\n"
          ]
        }
      ],
      "source": [
        "# 예2: 7일 확률이 가장 높다(0.6)\n",
        "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0] \n",
        "print(cross_entropy_error(np.array(y), np.array(t))) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "721bc4a7",
      "metadata": {},
      "source": [
        "- 예1 오차: 0.5 <-> 예2 오차: 2.3\n",
        "- 결과(오차 값)이 더 낮은 첫 번째 추정이 정답일 가능성이 높다"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07ed61c2",
      "metadata": {},
      "source": [
        "### 4.2.4 미니배치 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04650f86",
      "metadata": {},
      "source": [
        "- 기계학습 문제: 훈련 데이터를 사용해 학습\n",
        "    - 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수 찾기\n",
        "    <br/> -> 모든 훈련 데이터를 대상으로 손실 함수 값을 구해야한다.(100개면 100개 모두)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb0f857e",
      "metadata": {},
      "source": [
        "<img src = \"deep_learning_images/e 4.3.png\" width = \"30%\" height = \"30%\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f88f2173",
      "metadata": {},
      "source": [
        "- N개의 데이터로 확장 -> 마지막에 N으로 나누어 정규화 -> '평균 손실 함수'\n",
        "- 데이터 개수와 관계없이 언제든 통일된 지표를 얻을 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9c73fbc",
      "metadata": {},
      "source": [
        "- 미니배치(mini-batch)\n",
        "    - 모든 데이터를 대상으로 손실 함수의 합을 구하는데 시간이 오래걸리는 문제 해결\n",
        "    - 데이터 일부를 추려 '근사치'로 이용\n",
        "    - ex) 60,000장 -> 100장 무작위로 뽑아 학습(미니배치 학습)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f22d8435",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(60000, 10)\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = \\\n",
        "    load_mnist(normalize=True, one_hot_label=True) # 원-핫 인코딩\n",
        "\n",
        "print(x_train.shape) # (60000, 784)\n",
        "print(t_train.shape) # (60000, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "37c7d292",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_size = x_train.shape[0]\n",
        "batch_size = 10\n",
        "batch_mask = np.random.choice(train_size, batch_size) # np.random.choice(): 무작위로 (60000, 10)10장\n",
        "x_batch = x_train[batch_mask]\n",
        "t_batch = t_train[batch_mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "907a45dd",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([17120, 34932, 29011, 29171, 39508, 58198, 30628,  9171, 29287,\n",
              "       16574])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.random.choice(60000, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04b5e9c3",
      "metadata": {},
      "source": [
        "### 4.2.4 (배치용) 교차 엔트로피 오차 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "22abdeb0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# y: 신경망 출력\n",
        "# t: 정답 레이블\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1: # y가 1차원이라면(데이터 하나당 교차 엔트로피 오차를 구하는 경우)\n",
        "        t = t.reshape(1, t.size) # 데이터 형상 바꿈\n",
        "        y = y.reshape(1, y.size)\n",
        "    \n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(t * np.log(y + 1e-7)) / batch_size  # 배치 크기로 나누어 정규화\n",
        "    # one-hot_label이 False일 경우\n",
        "    # return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95814535",
      "metadata": {},
      "source": [
        "- np.log(y[np.arange(batch_size), t])\n",
        "    - np.arange(batch_size): 0 ~ batch_size-1 까지 배열 생성\n",
        "    - ex) batch_size: 5 -> [0, 1, 2, 3, 4] 넘파이 배열 생성\n",
        "    - t -> [2, 7, 0, 9, 4] 레이블 저장\n",
        "    - 각 데이터의 정답 레이블에 해당하는 신경망의 출력 추출([ y[0,2], y[1,7], y[2,0], y[3,9], y[4,4] ])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b5b69f5",
      "metadata": {},
      "source": [
        "### 4.2.5 왜 손실 함수를 설정하는가?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4b12638",
      "metadata": {},
      "source": [
        "- 왜 '정확도'가 아닌 '손실 함수의 값'?\n",
        "    - '미분'의 역할 주목\n",
        "    - 신경망 학습: 최적의 매개변수(가중치와 편향)를 탐색할 때 손실 함수의 값을 가능한 한 작게 하는 매개변수 값을 찾는다.\n",
        "    <br/>-> 매개변수의 미분(정확히는 기울기)을 계산하고, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복\n",
        "    - 미분 값 0 -> 가중치 매개변수의 갱신은 멈춘다.\n",
        "    \n",
        "         \n",
        "\n",
        "> 신경망을 학습할 때 정확도를 지표로 삼아서는 안 된다. 정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "707dc820",
      "metadata": {},
      "source": [
        "<img src = \"deep_learning_images/fig 4-4.png\" width = \"60%\" height = \"60%\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1b9b77c",
      "metadata": {},
      "source": [
        "- 정확도는 매개변수의 미소한 변화에는 거의 반응을 보이지 않고, 반응이 있더라도 그 값이 불연속적으로 갑자기 변화한다. -> '계단 함수'를 활성화 함수로 사용하지 않는 이유\n",
        "- '시그모이드 함수'의 미분은 어느 장소라도 0이 되지는 않는다. -> 신경망이 올바르게 학습 가능"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1a29e17",
      "metadata": {},
      "source": [
        "## 4.3 수치 미분"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7416518",
      "metadata": {},
      "source": [
        "### 4.3.1 미분"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d091f027",
      "metadata": {},
      "source": [
        "<img src = \"deep_learning_images/e 4.4.png\" width = \"30%\" height = \"30%\" >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6d0246a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 나쁜 구현 예\n",
        "def numerical_diff(f, x): # 수치 미분\n",
        "    h = 10e-50 # 0.00...1\n",
        "    return (f(x+h) - f(x)) / h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3e13beef",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.float32(1e-50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97ed321d",
      "metadata": {},
      "source": [
        "- 개선점\n",
        "    1. **반올림 오차(rounding error)문제:** 작은 값(가령 소수점 8자리 이하)이 생략되어 최종 계산 결과에 오차가 생김\n",
        "    2. **함수 f의 차분(finite difference):** h를 무한히 0으로 좁히는 것이 불가능해 생기는 한계 해결"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "848fba52",
      "metadata": {},
      "source": [
        "- 진정한 미분(진정한 접선)과 수치 미분(근사로 구한 접선)의 값은 다르다\n",
        "\n",
        "<img src = \"deep_learning_images/fig 4-5.png\" width = \"50%\" height = \"50%\" >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8f104733",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 개선\n",
        "def numerical_diff(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    return (f(x+h) - f(x-h)) / (2*h) # 중심 차분 or 중앙 차분 / 전방 차분"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0633433b",
      "metadata": {},
      "source": [
        "> 해석적(analytic)으로 미분하다: 수식 전개<br/>\n",
        "> 수치 미분(numerical differentiation): 근사치로 계산"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5519d5f",
      "metadata": {},
      "source": [
        "### 4.3.2 수치 미분의 예"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1461ee2",
      "metadata": {},
      "source": [
        "<img src = \"deep_learning_images/e 4.5.png\" width = \"30%\" height = \"30%\" >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "bb88c185",
      "metadata": {},
      "outputs": [],
      "source": [
        "def function_1(x):\n",
        "    return 0.01*x**2 + 0.1*x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "7a1e78dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "fa0d1085",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "x = np.arange(0.0, 20.0, 0.1)\n",
        "y = function_1(x)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "5a37e1e8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.1999999999990898\n",
            "0.2999999999986347\n"
          ]
        }
      ],
      "source": [
        "# x=5 일 때 미분, 0.1999999999990898 / 진정한 미분 값(해석학적) 0.2\n",
        "print(numerical_diff(function_1, 5))\n",
        "# x=10 일 때 미분, 0.2999999999986347 / 진정한 미분 값(해석학적) 0.3\n",
        "print(numerical_diff(function_1, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a87c7efe",
      "metadata": {},
      "source": [
        "### 4.3.3 편미분"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1e1e5d5",
      "metadata": {},
      "source": [
        "- **편미분:** 변수가 여럿인 함수에 대한 미분\n",
        "\n",
        "<img src = \"deep_learning_images/e 4.6.png\" width = \"30%\" height = \"30%\" >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "01ac8b4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def function_2(x):\n",
        "    return x[0]**2 + x[1]**2\n",
        "    # 또는 return np.sum(x**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27f5bffa",
      "metadata": {},
      "source": [
        "<img src = \"deep_learning_images/fig 4-8.png\" width = \"50%\" height = \"50%\" >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "b38a5b58",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.00000000000378\n"
          ]
        }
      ],
      "source": [
        "# x0 = 3, x1 = 4일 때, x0에 대한 편미분을 구하라.\n",
        "def function_tmp1(x0):\n",
        "    return x0*x0 + 4.0**2.0\n",
        "\n",
        "print(numerical_diff(function_tmp1, 3.0))\n",
        "# 6.00000000000378"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "6203e16d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7.999999999999119\n"
          ]
        }
      ],
      "source": [
        "# x0=3, x1=4일 때, x1에 대한 편미분을 구하라.\n",
        "def function_tmp2(x1):\n",
        "    return 3.0**2.0 + x1*x1\n",
        "\n",
        "print(numerical_diff(function_tmp2, 4.0))\n",
        "# 7.999999999999119"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "129ad879",
      "metadata": {},
      "source": [
        "- 여러 변수 중 목표 변수 하나에 초점을 맞추고 다른 변수는 값을 고정."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4832e2c",
      "metadata": {},
      "source": [
        "### 4.4 기울기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a89a792f",
      "metadata": {},
      "source": [
        "- x0과 x1의 편미분을 동시에 계산하고 싶다면?\n",
        "- **기울기(gradient):** 모든 변수의 편미분을 벡터로 정리한 것"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "13c90398",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 기울기 구하기\n",
        "def numerical_gradient(f, x): # 넘파이 배열 x의 각원소에 대해서 수치 미분 구하기\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x) # x와 형상이 같고 그 원소가 모두 0인 배열\n",
        "\n",
        "    for idx in range(x.size):\n",
        "        tmp_val = x[idx]\n",
        "        # f(x+h) 계산\n",
        "        x[idx] = tmp_val + h\n",
        "        fxh1 = f(x)\n",
        "\n",
        "        # f(x-h) 계산\n",
        "        x[idx] = tmp_val - h\n",
        "        fxh2 = f(x)\n",
        "\n",
        "        grad = (fxh1 - fxh2) / (2*h)\n",
        "        x[idx] = tmp_val # 값 복원\n",
        "    \n",
        "    return grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "95e0ea78",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7.999999999999119\n",
            "4.000000000004\n",
            "0.0\n"
          ]
        }
      ],
      "source": [
        "print(numerical_gradient(function_2, np.array([3.0,4.0]))) # (3,4) 기울기, 7.999999999999119\n",
        "print(numerical_gradient(function_2, np.array([0.0,2.0]))) # (0,2) 기울기, 4.000000000004\n",
        "print(numerical_gradient(function_2, np.array([3.0,0.0]))) # (3,0) 기울기, 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f14cf1b",
      "metadata": {},
      "source": [
        "- 기울기의 결과에 마이너스를 붙인 벡터 그림\n",
        "\n",
        "<img src = \"deep_learning_images/fig 4-9.png\" width = \"50%\" height = \"50%\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6731910a",
      "metadata": {},
      "source": [
        "- 기울기의 의미\n",
        "    - 함수의 '가장 낮은 장소(최솟값)'를 가리키는 것 같음 + '가장 낮은 곳'에서 멀어질수록 화살표의 크기가 커짐\n",
        "    - 기울기는 가장 낮은 장소를 가리키지만, 실제는 반드시 그렇다고 할 수 없다.\n",
        "    - **기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향**이다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "073f41d0",
      "metadata": {},
      "source": [
        "### 4.4.1 경사법(경사 하강법)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84c5d42a",
      "metadata": {},
      "source": [
        "- **경사법(gradient method):** 기울기를 잘 이용해 함수의 최솟값(또는 가능한 한 작은 값)을 찾으려는 것, 함수의 값을 점차 줄이는 것\n",
        "- BUT, 각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표가 기울기. 가리키는 곳에 정말 함수의 최솟값이 있는지, 정말로 나아갈 방향인지는 보장할 수 없다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "941acd96",
      "metadata": {},
      "source": [
        "<img src = \"deep_learning_images/e 4.7.png\" width = \"30%\" height = \"30%\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "172f96ff",
      "metadata": {},
      "source": [
        "- 기호 에타(eta): 갱신하는 양 -> **학습률(learning rate)**\n",
        "    - 한 번의 학습으로 얼마만큼 학습해야 할지, 즉 매개변수 값을 얼마나 갱신하느냐를 정하는 것이 학습률\n",
        "- 변수의 값을 갱신하는 단계를 여러 번 반복하면서 서서히 함수의 값을 줄이는 것"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f6440922",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 경사 하강법\n",
        "# f: 최적화 하려는 함수\n",
        "# init_x: 초깃값\n",
        "# lr: 학습률(learning rate)\n",
        "# step_num: 반복 횟수\n",
        "\n",
        "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
        "    x = init_x\n",
        "\n",
        "    for i in range(step_num):\n",
        "        grad = numerical_gradient(f, x) # 기울기 구하기\n",
        "        x -= lr * grad # 갱신: 학습률x기울기\n",
        "    \n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "5a57d040",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-7.00000000e+00  8.12235612e-10]\n"
          ]
        }
      ],
      "source": [
        "# 경사법으로 f(x0,x1)=(x0^2)+(x1^2)의 최솟값을 구하라\n",
        "def function_2(x):\n",
        "    return x[0]**2 + x[1]**2\n",
        "\n",
        "# 경사 하강법으로 최솟값 구하기\n",
        "init_x = np.array([-3.0, 4.0]) # 초깃값 설정\n",
        "print(gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100))\n",
        "# [-7.00000000e+00  8.12235612e-10] -> 거의 0,0에 가까운 결과"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c2a11a5",
      "metadata": {},
      "source": [
        "- 경사법에 의한 갱신 과정, 점선은 함수의 등고선을 나타낸다\n",
        "\n",
        "<img src = \"deep_learning_images/fig 4-10.png\" width = \"50%\" height = \"50%\" >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "f45f2d7d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-1.29504382e+12 -1.29504382e+12]\n"
          ]
        }
      ],
      "source": [
        "# 학습률이 너무 크다면? lr=10.0\n",
        "init_x = np.array([-3.0, 4.0])\n",
        "print(gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100))\n",
        "# [-1.29504382e+12 -1.29504382e+12] -> 발산"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "b5607545",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-3.00000008  3.99999992]\n"
          ]
        }
      ],
      "source": [
        "# 학습률이 너무 작다면? lr=1e-10\n",
        "init_x = np.array([-3.0, 4.0])\n",
        "print(gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100))\n",
        "# [-3.00000008  3.99999992] -> 거의 갱신X"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d162e977",
      "metadata": {},
      "source": [
        "> 학습률 같은 매개변수를 **하이퍼파라미터(hyper parameter)**라고 한다. 이는 가중치와 편향과 같은 신경망의 매개변수와는 성질이 다르다. 신경망의 가중치, 편향은 훈련 데이터와 학습 알고리즘에 의해서 '자동'으로 획득되는 매개변수인 반면, 학습률 같은 하이퍼파라미터는 사람이 직접 설정해야 하는 매개변수이다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "770cc483",
      "metadata": {},
      "source": [
        "### 4.4.2 신경망에서의 기울기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68fd6fc2",
      "metadata": {},
      "source": [
        "- 신경망 학습에서 기울기 구하기: 가중치 매개변수에 대한 손실함수의 기울기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ec043dc",
      "metadata": {},
      "source": [
        "- 형상 2x3\n",
        "- 가중치 W\n",
        "- 손실함수 L"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe7ea5cc",
      "metadata": {},
      "source": [
        "<img src = \"deep_learning_images/e 4.8.png\" width = \"30%\" height = \"30%\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d62c4d47",
      "metadata": {},
      "source": [
        "- 경사: 각각의 원소에 관한 편미분: 각 원소를 조금 변경했을 때 손실 함수 L이 얼마나 변화하느냐\n",
        "- 가중치와 경사의 형상 동일: 2x3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b116fde2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from common.functions import softmax, cross_entropy_error\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "class simpleNet:\n",
        "    def __init__(self):\n",
        "        self.W = np.random.randn(2,3) # 정규분포로 초기화 2X3\n",
        "    \n",
        "    # 예측\n",
        "    def predict(self, x):\n",
        "        return np.dot(x, self.W)\n",
        "\n",
        "    # 손실 함수의 값\n",
        "    def loss(self, x, t):\n",
        "        z = self.predict(x)\n",
        "        y = softmax(z)\n",
        "        loss = cross_entropy_error(y, t) # 정답과 오차\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "61a17525",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-1.09677253 -0.21046943  0.69282443]\n",
            " [ 0.09116778 -0.80885897 -0.6083582 ]]\n"
          ]
        }
      ],
      "source": [
        "net = simpleNet()\n",
        "print(net.W) # 가중치 매개변수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "26e98b59",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.57601252 -0.85425474 -0.13182773]\n"
          ]
        }
      ],
      "source": [
        "x = np.array([0.6, 0.9])\n",
        "p = net.predict(x)\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "36e2b5b3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "print(np.argmax(p)) # 최댓값 인덱스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "e0e61c8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "t = np.array([0,1,0]) # 정답 레이블"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "4364cae3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.4771011349018117"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net.loss(x, t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "2182b668",
      "metadata": {},
      "outputs": [],
      "source": [
        "def f(W):\n",
        "    return net.loss(x, t)\n",
        "# f = lambda w: net.loss(x,t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "ad14fe4b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.1809227  -0.46302074  0.28209804]\n",
            " [ 0.27138405 -0.6945311   0.42314705]]\n"
          ]
        }
      ],
      "source": [
        "dW = numerical_gradient(f, net.W) # 다차원 배열 처리 가능하도록 함수 수정.\n",
        "print(dW)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "287ca6cc",
      "metadata": {},
      "source": [
        "- w11을 h만큼 늘리면 -> 손실 함수의 값은 0.02만큼 증가\n",
        "- w22을 h만큼 늘리면 -> 손실 한수의 값은 0.09만큼 감소\n",
        "- 손실 함수를 줄인다는 관점에서 w00은 음의 방향으로 갱신 / w11은 양의 방향으로 갱신"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e8a8744",
      "metadata": {},
      "source": [
        "## 4.5 학습 알고리즘 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f249de18",
      "metadata": {},
      "source": [
        "- 신경망 학습의 절차\n",
        "    - 전제:<br/>신경망에는 적용 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다. 신경망 학습은 다음과 같이 4단계로 수행한다.\n",
        "        \n",
        "    1. 미니배치:<br/> 훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표다.\n",
        "    <br/> **-> 확률적 경사 하강법(stochastic gradient descent, SGD)**\n",
        "    2. 기울기 산출:<br/> 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n",
        "    3. 매개변수 갱신:<br/> 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
        "    4. 반복:<br/> 1 ~ 3단계를 반복한다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56018ddd",
      "metadata": {},
      "source": [
        "### 4.5.1 2층 신경망 클래스 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "917d58c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "from common.functions import *\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "# 2층 신경망 클래스 TwoLayerNet\n",
        "class TwoLayerNet:\n",
        "    # 초기화 수행\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "        \n",
        "        # 가중치 초기화: 정규분포 난수, 편향은 0으로\n",
        "        self.params = {} # 매개변수 보관 딕셔너리\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "    \n",
        "    # 예측(추론) 수행\n",
        "    def predict(self, x):\n",
        "        \n",
        "        W1, W2 = self.params['W1'], self.params['W2']\n",
        "        b1, b2 = self.params['b1'], self.params['b2']\n",
        "        \n",
        "        a1 = np.dot(x, W1) + b1\n",
        "        z1 = sigmoid(a1)\n",
        "        a2 = np.dot(z1, W2) + b2\n",
        "        y = softmax(a2)\n",
        "        \n",
        "        return y\n",
        "    \n",
        "    # 손실 함수의 값 구하기: predict()의 결과와 정답 레이블을 바탕으로 교차 엔트로피 오차\n",
        "    # x: 입력 데이터, t: 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        \n",
        "        return cross_entropy_error(y, t)\n",
        "    \n",
        "    # 정확도 구하기\n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        t = np.argmax(t, axis=1)\n",
        "        \n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "    \n",
        "    # 가중치 매개변수의 기울기 구하기(수치 미분 방식) / 고속(오차역전파법)\n",
        "    # x: 입력 데이터, t: 정답 레이블\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "        \n",
        "        grads = {} # 기울기 보관 딕셔너리 \n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "        \n",
        "        return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "f2ae155e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(784, 100)\n",
            "(100,)\n",
            "(100, 10)\n",
            "(10,)\n"
          ]
        }
      ],
      "source": [
        "# params -> 가중치 매개변수 저장\n",
        "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
        "print(net.params['W1'].shape) # (784, 100)\n",
        "print(net.params['b1'].shape) # (100,)\n",
        "print(net.params['W2'].shape) # (100, 10)\n",
        "print(net.params['b2'].shape) # (10,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "e8b5472f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.09753395, 0.09413259, 0.10299772, 0.10052944, 0.10504186,\n",
              "        0.10068374, 0.09883676, 0.10504423, 0.09510377, 0.10009595],\n",
              "       [0.09764982, 0.09449404, 0.10307961, 0.100513  , 0.10477806,\n",
              "        0.10036883, 0.09904291, 0.10515957, 0.09498776, 0.09992638],\n",
              "       [0.09785843, 0.09456273, 0.10305851, 0.10038981, 0.10425921,\n",
              "        0.1002804 , 0.09916164, 0.10521567, 0.09529394, 0.09991966],\n",
              "       [0.09709031, 0.09470631, 0.10311594, 0.1005091 , 0.10464903,\n",
              "        0.10047443, 0.0992642 , 0.10515632, 0.09499112, 0.10004325],\n",
              "       [0.09784577, 0.09448052, 0.10325864, 0.10054253, 0.10414802,\n",
              "        0.10077648, 0.09901845, 0.10482894, 0.09527958, 0.09982106],\n",
              "       [0.0974045 , 0.09438032, 0.10287977, 0.10031865, 0.10460088,\n",
              "        0.10089077, 0.09906721, 0.10520975, 0.09538221, 0.09986593],\n",
              "       [0.09804975, 0.09454829, 0.10262466, 0.10037515, 0.10462443,\n",
              "        0.10038483, 0.09907872, 0.10513959, 0.09526516, 0.09990944],\n",
              "       [0.09762769, 0.09478656, 0.10339291, 0.10018104, 0.10425615,\n",
              "        0.10043942, 0.09914814, 0.10488679, 0.09507323, 0.10020806],\n",
              "       [0.09800843, 0.09433301, 0.1031688 , 0.10056247, 0.10450684,\n",
              "        0.10055931, 0.09905564, 0.10477575, 0.09510127, 0.09992848],\n",
              "       [0.09755958, 0.09476444, 0.10289992, 0.10050162, 0.10475045,\n",
              "        0.10061423, 0.09938872, 0.1047067 , 0.09495052, 0.09986382],\n",
              "       [0.09791644, 0.09473574, 0.10273422, 0.1007327 , 0.10436999,\n",
              "        0.10048062, 0.09931517, 0.10490187, 0.09496686, 0.0998464 ],\n",
              "       [0.09780549, 0.09451155, 0.10258198, 0.10047224, 0.10455756,\n",
              "        0.10040512, 0.09948056, 0.10519227, 0.09521123, 0.099782  ],\n",
              "       [0.09786158, 0.09479995, 0.10268483, 0.10071584, 0.1045935 ,\n",
              "        0.10019228, 0.09931647, 0.10512846, 0.09516139, 0.0995457 ],\n",
              "       [0.09770674, 0.09436617, 0.10274262, 0.10047744, 0.10434583,\n",
              "        0.10083089, 0.09915408, 0.10507107, 0.09516992, 0.10013524],\n",
              "       [0.09777332, 0.09454456, 0.10274473, 0.10040614, 0.10491156,\n",
              "        0.10062459, 0.09897125, 0.10515458, 0.09501394, 0.09985533],\n",
              "       [0.0981562 , 0.09443096, 0.10253736, 0.10016794, 0.10453577,\n",
              "        0.10049672, 0.09938894, 0.10499195, 0.09507676, 0.1002174 ],\n",
              "       [0.09761521, 0.09454869, 0.1033224 , 0.10060205, 0.10468238,\n",
              "        0.10043394, 0.0989863 , 0.10480419, 0.09540537, 0.09959947],\n",
              "       [0.09789075, 0.09413142, 0.10287935, 0.10062867, 0.1045563 ,\n",
              "        0.10101018, 0.0991439 , 0.10493703, 0.09492147, 0.09990093],\n",
              "       [0.09758502, 0.09467425, 0.10287322, 0.10056568, 0.10479691,\n",
              "        0.10046303, 0.09934256, 0.10543847, 0.09497194, 0.09928893],\n",
              "       [0.09759289, 0.09461929, 0.10306717, 0.1002631 , 0.1044518 ,\n",
              "        0.10088359, 0.09935923, 0.10498203, 0.09483842, 0.09994248],\n",
              "       [0.09757721, 0.09462305, 0.10304442, 0.10026131, 0.10447125,\n",
              "        0.10046617, 0.09940619, 0.10485676, 0.09517848, 0.10011517],\n",
              "       [0.09797342, 0.09436463, 0.10297978, 0.10033714, 0.10442592,\n",
              "        0.10057054, 0.09900035, 0.10507699, 0.09517624, 0.100095  ],\n",
              "       [0.09770585, 0.09454199, 0.10282967, 0.10049552, 0.10434965,\n",
              "        0.10078515, 0.09961995, 0.10514884, 0.09502413, 0.09949926],\n",
              "       [0.09783395, 0.09419415, 0.10306125, 0.10041183, 0.10450402,\n",
              "        0.10052034, 0.09905234, 0.10532223, 0.09494169, 0.1001582 ],\n",
              "       [0.09784421, 0.09461427, 0.10271824, 0.10066907, 0.10432788,\n",
              "        0.10055218, 0.09964811, 0.10489264, 0.09473085, 0.10000255],\n",
              "       [0.0974267 , 0.0948052 , 0.10269715, 0.10056348, 0.10444328,\n",
              "        0.10076083, 0.09923406, 0.10505089, 0.09527722, 0.09974117],\n",
              "       [0.09748088, 0.09463377, 0.10304956, 0.10047915, 0.104483  ,\n",
              "        0.10072085, 0.09947467, 0.1046944 , 0.09495842, 0.1000253 ],\n",
              "       [0.09782906, 0.09436439, 0.10317554, 0.10040308, 0.10438334,\n",
              "        0.10082689, 0.09932434, 0.10459718, 0.09486558, 0.10023059],\n",
              "       [0.09793726, 0.09455815, 0.10262009, 0.10062397, 0.10476174,\n",
              "        0.1005052 , 0.09909937, 0.10523008, 0.09499   , 0.09967414],\n",
              "       [0.09769202, 0.09447295, 0.10347331, 0.1006769 , 0.10454396,\n",
              "        0.10085639, 0.09897478, 0.10484905, 0.09491186, 0.09954877],\n",
              "       [0.09778581, 0.09441994, 0.10268264, 0.10092931, 0.10491758,\n",
              "        0.10055062, 0.09918005, 0.10468648, 0.09490064, 0.09994691],\n",
              "       [0.09753156, 0.09438206, 0.10309602, 0.10083742, 0.10467506,\n",
              "        0.10071658, 0.09856949, 0.10520599, 0.09510447, 0.09988134],\n",
              "       [0.09783646, 0.09456972, 0.10276007, 0.10044945, 0.1047531 ,\n",
              "        0.10041109, 0.09935522, 0.10496632, 0.09501043, 0.09988814],\n",
              "       [0.09769361, 0.09455144, 0.10328023, 0.10031781, 0.10429269,\n",
              "        0.10085193, 0.0991531 , 0.10507783, 0.09477399, 0.10000737],\n",
              "       [0.09778133, 0.09449362, 0.10287783, 0.10080562, 0.10459212,\n",
              "        0.10056392, 0.09883598, 0.10509308, 0.09478613, 0.10017037],\n",
              "       [0.09769684, 0.09440526, 0.10285297, 0.10072153, 0.10485145,\n",
              "        0.10056682, 0.09885582, 0.10485598, 0.09513852, 0.10005481],\n",
              "       [0.09798721, 0.09439379, 0.10287265, 0.10053347, 0.10446134,\n",
              "        0.10071176, 0.09908718, 0.10511497, 0.09521925, 0.09961837],\n",
              "       [0.09795791, 0.0943174 , 0.10277613, 0.10039414, 0.10441665,\n",
              "        0.10065677, 0.09957515, 0.10499513, 0.09498279, 0.09992792],\n",
              "       [0.09803411, 0.09435579, 0.10310833, 0.1000429 , 0.10444881,\n",
              "        0.10065363, 0.09930758, 0.10537214, 0.09502117, 0.09965553],\n",
              "       [0.09766755, 0.09450904, 0.10272974, 0.10093615, 0.10445848,\n",
              "        0.10041165, 0.09901833, 0.10537896, 0.09481995, 0.10007017],\n",
              "       [0.09757281, 0.09447999, 0.10260337, 0.10048928, 0.10456597,\n",
              "        0.10087048, 0.0992011 , 0.10512886, 0.09511579, 0.09997235],\n",
              "       [0.09746568, 0.09438193, 0.10267118, 0.10047961, 0.10462094,\n",
              "        0.10091105, 0.09906013, 0.10516549, 0.09507252, 0.10017146],\n",
              "       [0.09797468, 0.0947037 , 0.10280182, 0.1005429 , 0.10436263,\n",
              "        0.10059068, 0.09928615, 0.10498031, 0.09512962, 0.09962752],\n",
              "       [0.09749154, 0.09465947, 0.10315147, 0.10026568, 0.10489268,\n",
              "        0.10064866, 0.09895765, 0.1050584 , 0.09479952, 0.10007492],\n",
              "       [0.09737896, 0.0944639 , 0.10297042, 0.10051956, 0.10461059,\n",
              "        0.10084759, 0.09896242, 0.10493154, 0.09516081, 0.10015422],\n",
              "       [0.09803335, 0.09438438, 0.10285355, 0.10049654, 0.10454501,\n",
              "        0.10016484, 0.09892899, 0.1050665 , 0.09529402, 0.10023281],\n",
              "       [0.09776679, 0.09454303, 0.1027883 , 0.10078538, 0.10456677,\n",
              "        0.10060239, 0.09913959, 0.10525862, 0.09488469, 0.09966444],\n",
              "       [0.09779109, 0.09404899, 0.10256179, 0.10077691, 0.10438582,\n",
              "        0.10041539, 0.0990916 , 0.10561369, 0.09505517, 0.10025955],\n",
              "       [0.0980241 , 0.09435607, 0.10281913, 0.10054789, 0.10429214,\n",
              "        0.10053245, 0.09918001, 0.10513661, 0.09545068, 0.09966092],\n",
              "       [0.09788581, 0.0943518 , 0.10350605, 0.10037309, 0.10436702,\n",
              "        0.10018619, 0.09918714, 0.10524324, 0.09511848, 0.09978118],\n",
              "       [0.09757569, 0.09448335, 0.10307442, 0.10056688, 0.10454017,\n",
              "        0.10101468, 0.0989918 , 0.10492065, 0.09504486, 0.0997875 ],\n",
              "       [0.09753283, 0.09440075, 0.10302831, 0.10019909, 0.10454676,\n",
              "        0.10091903, 0.09929793, 0.10524143, 0.09521395, 0.09961992],\n",
              "       [0.09811683, 0.09446269, 0.10256855, 0.10022576, 0.10461204,\n",
              "        0.10038692, 0.09925391, 0.10530037, 0.09513709, 0.09993585],\n",
              "       [0.09752835, 0.09443472, 0.10288344, 0.10071659, 0.1046942 ,\n",
              "        0.10061915, 0.09927835, 0.10486215, 0.09503229, 0.09995077],\n",
              "       [0.09748008, 0.09464617, 0.10312114, 0.10073573, 0.10481277,\n",
              "        0.10037592, 0.09899185, 0.10477161, 0.09532063, 0.09974411],\n",
              "       [0.09761881, 0.09439119, 0.10303656, 0.10020503, 0.10478198,\n",
              "        0.10057981, 0.09904597, 0.10514739, 0.0951747 , 0.10001855],\n",
              "       [0.09762965, 0.09469117, 0.10301457, 0.10030381, 0.10473358,\n",
              "        0.10037938, 0.09957444, 0.10467315, 0.09486291, 0.10013732],\n",
              "       [0.09806392, 0.09417351, 0.10258772, 0.10079329, 0.10484306,\n",
              "        0.10067163, 0.09896744, 0.10496991, 0.09487704, 0.10005248],\n",
              "       [0.09754931, 0.09424153, 0.10280504, 0.10030257, 0.10486976,\n",
              "        0.10076192, 0.09920372, 0.10547688, 0.09478001, 0.10000926],\n",
              "       [0.09765719, 0.09456934, 0.10321276, 0.10057496, 0.10445663,\n",
              "        0.10072659, 0.0992821 , 0.10484726, 0.09478411, 0.09988905],\n",
              "       [0.09743709, 0.09447921, 0.10336657, 0.10038847, 0.10473911,\n",
              "        0.10065668, 0.09884004, 0.10520185, 0.09498762, 0.09990336],\n",
              "       [0.09780275, 0.09443113, 0.10291535, 0.10041437, 0.10452704,\n",
              "        0.10081841, 0.0989206 , 0.10524089, 0.09496809, 0.09996138],\n",
              "       [0.09753337, 0.09445768, 0.10289513, 0.10070958, 0.10463755,\n",
              "        0.10079826, 0.09876805, 0.10490305, 0.0952618 , 0.10003553],\n",
              "       [0.09750704, 0.09446944, 0.10308033, 0.10059685, 0.10469441,\n",
              "        0.10057897, 0.09914704, 0.10491727, 0.0951839 , 0.09982476],\n",
              "       [0.09771889, 0.09449746, 0.10278149, 0.10040785, 0.10466081,\n",
              "        0.10064377, 0.09905272, 0.10498055, 0.0955065 , 0.09974996],\n",
              "       [0.09762253, 0.09472076, 0.10294299, 0.10051834, 0.10484824,\n",
              "        0.10105919, 0.09914559, 0.10437087, 0.09480256, 0.09996891],\n",
              "       [0.09779002, 0.09433108, 0.10330417, 0.100513  , 0.10419362,\n",
              "        0.10092245, 0.0991425 , 0.10478985, 0.0952012 , 0.09981211],\n",
              "       [0.09743061, 0.09452401, 0.10304373, 0.10026568, 0.10485491,\n",
              "        0.10084648, 0.09913006, 0.10494565, 0.09526197, 0.0996969 ],\n",
              "       [0.09794119, 0.09399362, 0.10300757, 0.10030178, 0.10469897,\n",
              "        0.10083432, 0.09915799, 0.10488683, 0.0954315 , 0.09974623],\n",
              "       [0.09749673, 0.09442467, 0.10290909, 0.10085057, 0.1046187 ,\n",
              "        0.10073694, 0.0989153 , 0.10526757, 0.0949072 , 0.09987324],\n",
              "       [0.09755853, 0.09465667, 0.10290987, 0.10034837, 0.10457647,\n",
              "        0.10103456, 0.09926228, 0.10454768, 0.09512816, 0.09997741],\n",
              "       [0.09732124, 0.09455102, 0.10310294, 0.10033881, 0.10472289,\n",
              "        0.10100131, 0.09896207, 0.10501608, 0.0950769 , 0.09990673],\n",
              "       [0.09763502, 0.09439344, 0.1028524 , 0.10066819, 0.10451548,\n",
              "        0.10104809, 0.09885756, 0.10507319, 0.09505766, 0.09989896],\n",
              "       [0.09777515, 0.09443977, 0.10332787, 0.10022146, 0.10496438,\n",
              "        0.10044245, 0.09913772, 0.105103  , 0.09486721, 0.09972098],\n",
              "       [0.09794538, 0.0941509 , 0.10273263, 0.10083296, 0.10462593,\n",
              "        0.10070923, 0.09896567, 0.10526847, 0.09497877, 0.09979005],\n",
              "       [0.09769586, 0.09414336, 0.10296494, 0.10041499, 0.10487787,\n",
              "        0.10045987, 0.09901568, 0.10525013, 0.09508346, 0.10009384],\n",
              "       [0.09755439, 0.09436937, 0.10292949, 0.10022788, 0.10443711,\n",
              "        0.10068362, 0.09943921, 0.10548349, 0.0951688 , 0.09970662],\n",
              "       [0.0976925 , 0.09439126, 0.10312085, 0.10047748, 0.10437791,\n",
              "        0.10066242, 0.09904809, 0.10493638, 0.09513922, 0.1001539 ],\n",
              "       [0.09768349, 0.09466486, 0.10320854, 0.10056583, 0.10488374,\n",
              "        0.10083533, 0.09876114, 0.10486603, 0.09491461, 0.09961643],\n",
              "       [0.09779462, 0.09464569, 0.10299952, 0.10011429, 0.10448653,\n",
              "        0.10087128, 0.09932483, 0.10504247, 0.09506955, 0.09965122],\n",
              "       [0.09751783, 0.09438805, 0.10310567, 0.10066305, 0.10473885,\n",
              "        0.10054752, 0.09909117, 0.1049082 , 0.09514963, 0.09989001],\n",
              "       [0.09728258, 0.09467498, 0.10268487, 0.10057909, 0.10494949,\n",
              "        0.10047835, 0.09876045, 0.10522471, 0.09538691, 0.09997857],\n",
              "       [0.0974369 , 0.09461078, 0.10315481, 0.10008706, 0.10510123,\n",
              "        0.10056719, 0.09910064, 0.1050117 , 0.09494643, 0.09998326],\n",
              "       [0.09791869, 0.0943031 , 0.10298194, 0.10035199, 0.10471818,\n",
              "        0.10058042, 0.09902276, 0.10528076, 0.09509888, 0.09974329],\n",
              "       [0.09763914, 0.09430564, 0.10288973, 0.10060636, 0.10476471,\n",
              "        0.10092653, 0.09948095, 0.10468382, 0.09499881, 0.09970432],\n",
              "       [0.09814016, 0.09436343, 0.10300979, 0.1001074 , 0.10412292,\n",
              "        0.10085275, 0.0990116 , 0.10510887, 0.09493676, 0.10034634],\n",
              "       [0.09763189, 0.09437206, 0.10291282, 0.10022628, 0.10469469,\n",
              "        0.1007119 , 0.09933517, 0.10505986, 0.09522669, 0.09982863],\n",
              "       [0.09794778, 0.09433741, 0.10286838, 0.10065283, 0.10438565,\n",
              "        0.10039882, 0.09927414, 0.10516992, 0.09518889, 0.09977618],\n",
              "       [0.09765118, 0.09458599, 0.10301486, 0.1006223 , 0.10454209,\n",
              "        0.10076561, 0.09888229, 0.10482039, 0.09525038, 0.09986492],\n",
              "       [0.09782776, 0.09434219, 0.1027885 , 0.1006308 , 0.10468222,\n",
              "        0.10063715, 0.09928333, 0.10488021, 0.09535787, 0.09956998],\n",
              "       [0.09791437, 0.09430819, 0.10293292, 0.10070435, 0.10428825,\n",
              "        0.10068698, 0.09888634, 0.10530471, 0.09486869, 0.10010518],\n",
              "       [0.09791988, 0.09429597, 0.10261532, 0.10041711, 0.1050976 ,\n",
              "        0.10049952, 0.09918939, 0.10516826, 0.09482907, 0.0999679 ],\n",
              "       [0.09777675, 0.09429991, 0.1026889 , 0.10049225, 0.10456287,\n",
              "        0.10076278, 0.09881152, 0.10539488, 0.09556827, 0.09964187],\n",
              "       [0.09769043, 0.09459646, 0.10336312, 0.10048446, 0.10452447,\n",
              "        0.10055893, 0.09938541, 0.10453259, 0.09479893, 0.1000652 ],\n",
              "       [0.09732571, 0.094341  , 0.10297683, 0.10031219, 0.10485116,\n",
              "        0.10052598, 0.09942975, 0.10507219, 0.09533562, 0.09982958],\n",
              "       [0.09781279, 0.0946061 , 0.10314661, 0.10025313, 0.1045409 ,\n",
              "        0.10066756, 0.09928409, 0.1050706 , 0.09507195, 0.09954626],\n",
              "       [0.09767096, 0.09451784, 0.10270043, 0.10035414, 0.10501714,\n",
              "        0.10078922, 0.09917538, 0.10475145, 0.09511656, 0.09990688],\n",
              "       [0.09791847, 0.09430341, 0.10323702, 0.10062125, 0.10444759,\n",
              "        0.1010211 , 0.09884677, 0.10491178, 0.09502367, 0.09966894],\n",
              "       [0.0980328 , 0.09476625, 0.10290766, 0.1003051 , 0.10416726,\n",
              "        0.10052454, 0.09930084, 0.10479342, 0.09528109, 0.09992103],\n",
              "       [0.09762536, 0.0946355 , 0.10296538, 0.1005325 , 0.10462584,\n",
              "        0.10056555, 0.09888359, 0.10506656, 0.09508179, 0.10001794]])"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 예측처리(순방향 처리)에 사용\n",
        "x = np.random.rand(100, 784) # 더미 입력 데이터(100장 분량)\n",
        "y = net.predict(x)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ab1a111",
      "metadata": {},
      "outputs": [],
      "source": [
        "x = np.random.rand(100, 784) # 더미 입력 데이터(100장 분량)\n",
        "t = np.random.rand(100, 10) # 더미 정답 레이블(100장 분량)\n",
        "\n",
        "grads = net.numerical_gradient(x, t) # 기울기 계산\n",
        "\n",
        "print(grads['W1'].shape) # (784, 100)\n",
        "print(grads['b1'].shape) # (100, )\n",
        "print(grads['W2'].shape) # (100, 10)\n",
        "print(grads['b2'].shape) # (10, )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d67bebb0",
      "metadata": {},
      "source": [
        "### 4.5.2 미니배치 학습 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1733131a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "# from two_layer_net import TwoLayerNet\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "train_loss_list = []\n",
        "\n",
        "# 하이퍼파라미터\n",
        "iters_num = 10000 # 반복 횟수\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100 # 미니배치 크기\n",
        "learning_rate = 0.1\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # 미니배치 획득\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    # 기울기 계산\n",
        "    grad = network.numerical_gradient(x_batch, t_batch)\n",
        "    # grad = network.gradient(x_batch, t_batch) # 성능 개선판!\n",
        "    \n",
        "    # 매개변수 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "    # 학습 경과 기록\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    print(i) # 확인용"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59eec80d",
      "metadata": {},
      "source": [
        "- 손실 함수 값의 추이: 왼쪽 10,000회 반복 까지, 오른쪽 1,000회 반복까지\n",
        "\n",
        "<img src = \"deep_learning_images/fig 4-11.png\" width = \"70%\" height = \"70%\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d51e2e8e",
      "metadata": {},
      "source": [
        "### 4.5.3 시험 데이터로 평가하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a90d03f",
      "metadata": {},
      "source": [
        "- 손실 함수의 값이 작아지는 것은 신경망이 잘 학습하고 있다는 방증, but '오버피팅' 확인\n",
        "- 훈련 외의 데이터를 올바르게 인식하는지 확인\n",
        "- '범용'적인 능력 \n",
        "- 1에폭(epoch) 별로 훈련 데이터와 시험 데이터에 대한 정확도 기록"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3de0b8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "# from two_layer_net import TwoLayerNet\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "# 하이퍼파라미터\n",
        "iters_num = 10000 # 반복 횟수\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100 # 미니배치 크기\n",
        "learning_rate = 0.1\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "train_loss_list = []\n",
        "# 에폭 리스트 추가\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "# 1에폭당 반복 수(10,000 / 100 -> 100회)\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # 미니배치 획득\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    grad = network.numerical_gradient(x_batch, t_batch)\n",
        "    # grad = network.gradient(x_batch, t_batch) # 성능 개선판!\n",
        "\n",
        "    # 매개변수 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "    # 학습 경과 기록\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    # 1에폭당 정확도 계산\n",
        "    if i % iter_per_epoch == 0: # 100번 마다(1에폭 마다)\n",
        "        \n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        \n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        \n",
        "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "529aa61e",
      "metadata": {},
      "source": [
        "- 훈련 데이터와 시험 데이터에 대한 정확도 추이\n",
        "\n",
        "<img src = \"deep_learning_images/fig 4-12.png\" width = \"60%\" height = \"60%\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60d3397a",
      "metadata": {},
      "source": [
        "- 에폭이 진행될수록(학습이 진행될수록) 훈련 데이터와 시험 데이터를 사용하고 평가한 정확도가 모두 좋아지고 있다. + 두 정확도 차이가X(선 겹침) -> 오버피팅이 일어나지 않았다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ece310e",
      "metadata": {},
      "source": [
        "## 4.6 정리"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04c9fbe5",
      "metadata": {},
      "source": [
        "- 신경망 학습\n",
        "- '지표': 손실 함수\n",
        "- 신경망 학습의 목표: 손실 함수를 기준으로 그 값이 가장 작아지는 가중치 매개변수 값을 찾아내는 것\n",
        "- 찾는 수법: 경사법(함수의 기울기 이용)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4c148f6",
      "metadata": {},
      "source": [
        "> **이번 장에서 배운 내용**\n",
        "* 기계학습에서 사용하는 데이터셋은 훈련 데이터와 시험 데이터로 나눠 사용한다.\n",
        "* 훈련 데이터로 학습한 모델의 범용 능력을 시험 데이터로 평가한다.\n",
        "* 신경망 학습은 손실 함수를 지표로, 손실 함수의 값이 작아지는 방향으로 가중치 매개변수를 갱신한다.\n",
        "* 가중치 매개변수를 갱신할 때는 가중치 매개변수의 기울기를 이용하고, 기울어진 방향으로 가중치의 값을 갱신하는 작업을 반복한다.\n",
        "* 아주 작은 값을 주었을 때의 차분으로 미분하는 것을 수치 미분이라고 한다.\n",
        "* 수치 미분을 이용해 가중치 매개변수의 기울기를 구할 수 있다.\n",
        "* 수치 미분을 이용한 계산에는 시간이 걸리지만, 그 구현은 간단하다. 한편, 다음 장에서 구현하는 (다소 복잡한) 오차역전파법은 기울기를 고속으로 구할 수 있다."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}